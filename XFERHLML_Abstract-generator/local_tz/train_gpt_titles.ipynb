{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_gpt_titles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylQ1Xscs60t"
      },
      "source": [
        "Mostly re-using Nava's code to\n",
        "\n",
        "1.   Download data from arxiv\n",
        "2.   Tokenize using spacy\n",
        "3.   Build data batches using pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aIbifVkamw5",
        "outputId": "673da696-7816-4cf6-e3c6-d14bb7717140"
      },
      "source": [
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "TRAIN = True\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    %cd /content/drive/MyDrive/Documents/HLML/abstracts/local_tz/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Documents/HLML/abstracts/local_tz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zERKMV41uCZ_"
      },
      "source": [
        "**Downloading data from arxiv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYWAv6uyt_cL",
        "outputId": "ca66216d-2c2e-45bc-8dbc-9344fccceb3f"
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import urllib.request\n",
        "import feedparser\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=031645bb7ec30c3a3d77f48d1f26b22329a5eaeb7fff51e8e985de4adb710262\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.8 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEPjrX8OlL8b"
      },
      "source": [
        "**Retrieving and saving data from Arxiv API:**\n",
        "\n",
        "Collecting all entries which contain the keyword 'system', and saving titles, abstracts, authors, category and tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUtMH2cul1CY"
      },
      "source": [
        "Run this cell if the pickled DataFrame does not exist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojpWC6KVuI26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a813ede1-4a26-4de6-d98b-59f391dc10b2"
      },
      "source": [
        "base_url = 'http://export.arxiv.org/api/query?';\n",
        "\n",
        "search_query = 'all:system' \n",
        "col=['title', 'summary', 'authors', 'arxiv_primary_category', 'tags']\n",
        "\n",
        "# if we ask for too many entries at once, we won't be guaranteed to get all of them\n",
        "# hence we need to iterate through several pages of results to build up our dataset\n",
        "\n",
        "master_df = pd.DataFrame(columns=col)\n",
        "counter = 0\n",
        "\n",
        "while counter < 10**4:\n",
        "\n",
        "  start = counter                  \n",
        "  max_results = 200\n",
        "\n",
        "  query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
        "                                                      start,\n",
        "                                                      max_results)\n",
        "  response = urllib.request.urlopen(base_url+query).read()\n",
        "  feed = feedparser.parse(response)\n",
        "\n",
        "  data_dict = {}\n",
        "  for c in col:\n",
        "    abstract_list=[]\n",
        "    for entry in feed.entries:\n",
        "      abstract_list.append(entry.get(c))\n",
        "    data_dict[c] = abstract_list\n",
        "\n",
        "  data_df = pd.DataFrame(data_dict,columns=col)\n",
        "  counter += len(feed.entries)\n",
        "\n",
        "  if master_df.empty:\n",
        "    master_df = data_df\n",
        "  else:\n",
        "    master_df = master_df.append(data_df, ignore_index=True)\n",
        "\n",
        "print(master_df)\n",
        "master_df.to_pickle(\"datasets/arxiv_system_10000.pkl\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  title  ...                                               tags\n",
            "0     Compact integral manifolds of differential sys...  ...  [{'term': 'math.DS', 'scheme': 'http://arxiv.o...\n",
            "1          Morphisms of Networks of Hybrid Open Systems  ...  [{'term': 'math.DS', 'scheme': 'http://arxiv.o...\n",
            "2     First integrals of ordinary linear differentia...  ...  [{'term': 'math.DS', 'scheme': 'http://arxiv.o...\n",
            "3     Complex Systems + Systems Engineering = Comple...  ...  [{'term': 'cs.MA', 'scheme': 'http://arxiv.org...\n",
            "4            Systems of quotients of Lie triple systems  ...  [{'term': 'math.RA', 'scheme': 'http://arxiv.o...\n",
            "...                                                 ...  ...                                                ...\n",
            "9995  Simulations of fluctuations of quantum statist...  ...  [{'term': 'cond-mat.stat-mech', 'scheme': 'htt...\n",
            "9996  Measurements of the Yield Stress in Frictionle...  ...  [{'term': 'cond-mat.soft', 'scheme': 'http://a...\n",
            "9997  Exact relaxation in a class of non-equilibrium...  ...  [{'term': 'cond-mat.stat-mech', 'scheme': 'htt...\n",
            "9998  Knowledge in Multi-Agent Systems: Initial Conf...  ...  [{'term': 'cs.LO', 'scheme': 'http://arxiv.org...\n",
            "9999  Specifying and Implementing Security Policies ...  ...  [{'term': 'cs.CR', 'scheme': 'http://arxiv.org...\n",
            "\n",
            "[10000 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZQyPdzal_7I"
      },
      "source": [
        "This cell loads the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcgsQbcymDLM"
      },
      "source": [
        "master_df = pd.read_pickle(\"datasets/arxiv_system_10000.pkl\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2LPRv-as4rY"
      },
      "source": [
        "**Tokenize using spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5LM2NIzu--9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030404ec-e140-4d57-8676-29d683f1c2d6"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=4ed04397e66fe7b28ad2d6e9765771740e2afaff1cce7b140a375d15cb392cab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kq34tmlc/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcMSwpTv427"
      },
      "source": [
        "titles = master_df.title.str.replace(r'\\n', ' ')\n",
        "titles = titles.str.replace('  ', '')\n",
        "doc=[nlp.tokenizer(text.lower().strip()) for text in titles]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCRz5TWfwlWW"
      },
      "source": [
        "## manually constructing vocabulary\n",
        "class Vocabulary:\n",
        "    PAD_token = 0   # Used for padding short sentences\n",
        "    BOS_token = 1   # Beginning-of-sentence token\n",
        "    EOS_token = 2   # End-of-sentence token\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.num_words = 0\n",
        "        self.num_sentences = 0\n",
        "        self.longest_sentence = 0\n",
        "\n",
        "        ## add PAD, BOS, EOS tokens:\n",
        "        self.word2index['<PAD>'] = self.num_words\n",
        "        self.word2count['<PAD>'] = 1\n",
        "        self.index2word[self.num_words] = '<PAD>'\n",
        "        self.num_words += 1\n",
        "\n",
        "        self.word2index['<BOS>'] = self.num_words\n",
        "        self.word2count['<BOS>'] = 1\n",
        "        self.index2word[self.num_words] = '<BOS>'\n",
        "        self.num_words += 1\n",
        "\n",
        "        self.word2index['<EOS>'] = self.num_words\n",
        "        self.word2count['<EOS>'] = 1\n",
        "        self.index2word[self.num_words] = '<EOS>'\n",
        "        self.num_words += 1\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # First entry of word into vocabulary\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # Word exists; increase word count\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 0 \n",
        "        for word in [token.text for token in sentence]:\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            self.longest_sentence = sentence_len\n",
        "        self.num_sentences += 1\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jHu4X-pwpSI"
      },
      "source": [
        "voc=Vocabulary('abstracts')\n",
        "for sent in doc:\n",
        " \tvoc.add_sentence(sent)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvP3VYxA7Rwl"
      },
      "source": [
        "Input_list=[]\n",
        "for sample in range(len(doc)):\n",
        "\tInput_list.append([voc.to_index(\"<BOS>\")]+[voc.to_index(token.text) for token in doc[sample]]+[voc.to_index(\"<EOS>\")])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zy5ltFexHkr"
      },
      "source": [
        "**Building datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnOQ25X1IB4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acd2ddb-9bd6-41e8-f9c8-8a8c39b518e1"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "\n",
        "arxiv_dataset = CustomTextDataset(Input_list)\n",
        "data_len = len(Input_list)\n",
        "print(data_len)\n",
        "train_list, validation_list, test_list = random_split(arxiv_dataset, [int(data_len*0.72), int(data_len*0.1), data_len-int(data_len*0.82)], generator=torch.Generator().manual_seed(42))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHsgmwyp3ieL"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for _sample in batch:\n",
        "        label_list.append(torch.tensor(_sample[:-1])) # data\n",
        "        text_list.append(torch.tensor(_sample[1:])) # trg\n",
        "    return pad_sequence(label_list, padding_value=0.0), pad_sequence(text_list, padding_value=0.0)\n",
        "\n",
        "batch_size = 30\n",
        "\n",
        "def create_iterators(batch_size=batch_size):\n",
        "    \"\"\"Heler function to create the iterators\"\"\"\n",
        "    dataloaders = []\n",
        "    for split in [train_list, validation_list, test_list]:\n",
        "        dataloader = DataLoader(\n",
        "            split, batch_size=batch_size,\n",
        "            collate_fn=collate_batch\n",
        "            )\n",
        "        dataloaders.append(dataloader)\n",
        "    return dataloaders"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwmmncuZ3-IM"
      },
      "source": [
        "**Make model and train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cz5g2onvyg5"
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from misc_functions import attention, make_std_mask\n",
        "from gpt_model import *\n",
        "import math, copy, time"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agPcjI_P4IeO"
      },
      "source": [
        "def make_model(vocab, N=12, \n",
        "\t\t\t   d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "\t\"\"\"Helper: Construct a model from hyperparameters.\"\"\"\n",
        "\n",
        "\t## returns EncoderDecoder object\n",
        "\tc = copy.deepcopy\n",
        "\tattn = MultiHeadedAttention(h, d_model)\n",
        "\tff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\tposition = PositionalEncoding(d_model, dropout)\n",
        "\tmodel = GPT(Decoder(DecoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "\t\t## Sequential passes input to the forward() method in the first module it stores\n",
        "\t\t## and then \"chains\" outputs to inputs sequentially for subsequent modules,\n",
        "\t\tnn.Sequential(Embeddings(d_model, vocab), c(position)),\n",
        "\t\tGenerator(d_model, vocab))\n",
        "\t\n",
        "\t# This was important from their code. \n",
        "\t# Initialize parameters with Glorot / fan_avg.\n",
        "\tfor p in model.parameters():\n",
        "\t\tif p.dim() > 1:\n",
        "\t\t\tnn.init.xavier_uniform_(p) # what does this do? How does it modify model?\n",
        "\treturn model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd6HiX9M4icE"
      },
      "source": [
        "Optimizer, loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApyssFBh4PUT"
      },
      "source": [
        "class NoamOpt:\n",
        "\t#\"Optim wrapper that implements rate.\"\n",
        "\tdef __init__(self, model_size, factor, warmup, optimizer):\n",
        "\t\tself.optimizer = optimizer\n",
        "\t\tself._step = 0\n",
        "\t\tself.warmup = warmup\n",
        "\t\tself.factor = factor\n",
        "\t\tself.model_size = model_size\n",
        "\t\tself._rate = 0\n",
        "\t\t\n",
        "\tdef step(self):\n",
        "\t\t# \"Update parameters and rate\"\n",
        "\t\tself._step += 1\n",
        "\t\trate = self.rate()\n",
        "\t\tfor p in self.optimizer.param_groups:\n",
        "\t\t\tp['lr'] = rate\n",
        "\t\tself._rate = rate\n",
        "\t\tself.optimizer.step()\n",
        "\t\t\n",
        "\tdef rate(self, step = None):\n",
        "\t\t# \"Implement `lrate` above\"\n",
        "\t\tif step is None:\n",
        "\t\t\tstep = self._step\n",
        "\t\treturn self.factor * \\\n",
        "\t\t\t(self.model_size ** (-0.5) *\n",
        "\t\t\tmin(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "\t# \"Implement label smoothing.\"\n",
        "\tdef __init__(self, size, padding_idx, smoothing=0.0):\n",
        "\t\tsuper(LabelSmoothing, self).__init__()\n",
        "\t\tself.criterion = nn.KLDivLoss(size_average=False) # Kullback-Leibler divergence loss\n",
        "\t\tself.padding_idx = padding_idx\n",
        "\t\tself.confidence = 1.0 - smoothing\n",
        "\t\tself.smoothing = smoothing\n",
        "\t\tself.size = size\n",
        "\t\tself.true_dist = None\n",
        "\t\t\n",
        "\tdef forward(self, x, target):\n",
        "\t\tassert x.size(1) == self.size\n",
        "\t\ttrue_dist = x.data.clone()\n",
        "\t\ttrue_dist.fill_(self.smoothing / (self.size - 2))\n",
        "\t\ttrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "\t\ttrue_dist[:, self.padding_idx] = 0\n",
        "\t\tmask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "\t\tif mask.dim() > 0:\n",
        "\t\t\ttrue_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\t\tself.true_dist = true_dist.requires_grad_(False)\n",
        "\n",
        "\t\treturn self.criterion(x, true_dist)\n",
        "  \n",
        "  \n",
        "class SimpleLossCompute:\n",
        "\t# \"A simple loss compute and train function.\"\n",
        "\tdef __init__(self, generator, criterion, opt=None):\n",
        "\t\tself.generator = generator\n",
        "\t\tself.criterion = criterion # LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "\t\tself.opt = opt # NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\t\t\n",
        "\tdef __call__(self, x, y, norm):\n",
        "\t\tx = self.generator(x) # x is output, each element now in d_vocab dimensions, shape = [30, 9, 11]\n",
        "\t\t\t\t\t\t\t  # y is batch.trg_y (first column of 1s removed), shape = [30, 9]\n",
        "\t\t\t\t\t\t\t  # norm is batch.ntokens (270)\n",
        "\t\t\n",
        "\t\tloss = self.criterion(x.contiguous().view(-1, x.size(-1)), # shape = [270, 11]\n",
        "\t\t\t\t\t\t\t  y.contiguous().view(-1)) / norm # shape = [270]\n",
        "\t\t# print(\"Label Smoothing called\")\n",
        "\t\tloss.backward()\n",
        "\t\tif self.opt is not None:\n",
        "\t\t\tself.opt.step()\n",
        "\t\t\tself.opt.optimizer.zero_grad()\n",
        "\n",
        "\t\tif list(loss.data.size()) != []:\n",
        "\t\t\treturn loss.data[0] * norm\n",
        "\t\telse:\n",
        "\t\t\treturn loss.data * norm\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U37LcJ1F4ovG"
      },
      "source": [
        "Make model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXdjCgSc4r5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4847c05d-95d6-4714-efa6-5dbea4cd73b7"
      },
      "source": [
        "V = voc.num_words\n",
        "model = make_model(V, N=12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWeHuwvo5TAO"
      },
      "source": [
        "def run_epoch(data_iterator, model, loss_compute):\n",
        "\t\"\"\"Standard Training and Logging Function\"\"\"\n",
        "\tstart = time.time()\n",
        "\ttotal_tokens = 0\n",
        "\ttotal_loss = 0\n",
        "\ttokens = 0\n",
        " \n",
        "\tfor i, batch in enumerate(data_iterator):\n",
        "\t\tdata = batch[0].T\n",
        "\t\ttrg = batch[1].T\n",
        "\t\tmask = make_std_mask(trg, pad=0)\n",
        "\t\tout = model.forward(data, mask)\n",
        "\t\tntokens = (trg != 0).data.sum()\n",
        "\t\tloss = loss_compute(out, trg, ntokens)\n",
        "\t\ttotal_loss += loss\n",
        "\t\ttotal_tokens += ntokens \n",
        "\t\ttokens += ntokens\n",
        "\t\tif i % 50 == 1:\n",
        "\t\t\telapsed = time.time() - start\n",
        "\t\t\tprint(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "\t\t\t\t\t(i, loss / ntokens, tokens / elapsed))\n",
        "\t\t\tstart = time.time()\n",
        "\t\t\ttokens = 0\n",
        "\treturn total_loss / total_tokens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQsug6PnUMLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e03d6f-79bc-43a9-d99a-2afe444b858f"
      },
      "source": [
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "## uses pytorch's Adam optimizer\n",
        "model_opt = NoamOpt(model.embed[0].d_model, 1, 4000,\n",
        "\t\ttorch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = create_iterators()\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  run_epoch(train_iterator, model, SimpleLossCompute(model.generator, criterion, model_opt))\n",
        "  model.eval() \n",
        "  run_epoch(valid_iterator, model, SimpleLossCompute(model.generator, criterion, None))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 2.026712 Tokens per Sec: 91.715179\n",
            "Epoch Step: 51 Loss: 2.529584 Tokens per Sec: 99.184006\n",
            "Epoch Step: 101 Loss: 2.204535 Tokens per Sec: 100.041481\n",
            "Epoch Step: 151 Loss: 1.757971 Tokens per Sec: 101.556664\n",
            "Epoch Step: 201 Loss: 1.203714 Tokens per Sec: 100.033089\n",
            "Epoch Step: 1 Loss: 1.667969 Tokens per Sec: 101.408867\n",
            "Epoch Step: 1 Loss: 1.967870 Tokens per Sec: 101.181755\n",
            "Epoch Step: 51 Loss: 2.038388 Tokens per Sec: 99.432251\n",
            "Epoch Step: 101 Loss: 1.587625 Tokens per Sec: 99.855103\n",
            "Epoch Step: 151 Loss: 1.530269 Tokens per Sec: 101.193123\n",
            "Epoch Step: 201 Loss: 1.156478 Tokens per Sec: 100.269638\n",
            "Epoch Step: 1 Loss: 1.502966 Tokens per Sec: 102.106621\n",
            "Epoch Step: 1 Loss: 1.928759 Tokens per Sec: 102.483704\n",
            "Epoch Step: 51 Loss: 1.834937 Tokens per Sec: 100.301697\n",
            "Epoch Step: 101 Loss: 1.393072 Tokens per Sec: 100.443863\n",
            "Epoch Step: 151 Loss: 1.297270 Tokens per Sec: 101.833923\n",
            "Epoch Step: 201 Loss: 1.098134 Tokens per Sec: 100.500153\n",
            "Epoch Step: 1 Loss: 1.303021 Tokens per Sec: 102.304688\n",
            "Epoch Step: 1 Loss: 1.715447 Tokens per Sec: 102.535042\n",
            "Epoch Step: 51 Loss: 1.582588 Tokens per Sec: 100.392509\n",
            "Epoch Step: 101 Loss: 1.301180 Tokens per Sec: 100.625694\n",
            "Epoch Step: 151 Loss: 1.282951 Tokens per Sec: 102.015076\n",
            "Epoch Step: 201 Loss: 1.040600 Tokens per Sec: 100.723419\n",
            "Epoch Step: 1 Loss: 1.141301 Tokens per Sec: 102.577484\n",
            "Epoch Step: 1 Loss: 1.552180 Tokens per Sec: 102.285133\n",
            "Epoch Step: 51 Loss: 1.450011 Tokens per Sec: 100.241096\n",
            "Epoch Step: 101 Loss: 1.268338 Tokens per Sec: 100.588821\n",
            "Epoch Step: 151 Loss: 1.218022 Tokens per Sec: 101.834122\n",
            "Epoch Step: 201 Loss: 0.983937 Tokens per Sec: 100.965912\n",
            "Epoch Step: 1 Loss: 1.077817 Tokens per Sec: 102.658546\n",
            "Epoch Step: 1 Loss: 1.548479 Tokens per Sec: 102.985741\n",
            "Epoch Step: 51 Loss: 1.402418 Tokens per Sec: 100.524391\n",
            "Epoch Step: 101 Loss: 1.187976 Tokens per Sec: 100.989540\n",
            "Epoch Step: 151 Loss: 1.153326 Tokens per Sec: 102.126999\n",
            "Epoch Step: 201 Loss: 0.944214 Tokens per Sec: 100.712723\n",
            "Epoch Step: 1 Loss: 1.011394 Tokens per Sec: 102.233429\n",
            "Epoch Step: 1 Loss: 1.367852 Tokens per Sec: 103.128822\n",
            "Epoch Step: 51 Loss: 1.367626 Tokens per Sec: 100.925873\n",
            "Epoch Step: 101 Loss: 1.132084 Tokens per Sec: 100.997681\n",
            "Epoch Step: 151 Loss: 1.080646 Tokens per Sec: 102.411140\n",
            "Epoch Step: 201 Loss: 0.935029 Tokens per Sec: 101.179520\n",
            "Epoch Step: 1 Loss: 0.992080 Tokens per Sec: 102.623726\n",
            "Epoch Step: 1 Loss: 1.307581 Tokens per Sec: 102.290077\n",
            "Epoch Step: 51 Loss: 1.348313 Tokens per Sec: 100.762421\n",
            "Epoch Step: 101 Loss: 1.045249 Tokens per Sec: 100.984138\n",
            "Epoch Step: 151 Loss: 1.056733 Tokens per Sec: 102.155464\n",
            "Epoch Step: 201 Loss: 0.959451 Tokens per Sec: 101.247620\n",
            "Epoch Step: 1 Loss: 0.973448 Tokens per Sec: 102.451118\n",
            "Epoch Step: 1 Loss: 1.190841 Tokens per Sec: 102.405190\n",
            "Epoch Step: 51 Loss: 1.229252 Tokens per Sec: 100.451332\n",
            "Epoch Step: 101 Loss: 1.106415 Tokens per Sec: 101.092018\n",
            "Epoch Step: 151 Loss: 1.019429 Tokens per Sec: 102.394150\n",
            "Epoch Step: 201 Loss: 0.886369 Tokens per Sec: 100.402740\n",
            "Epoch Step: 1 Loss: 0.887101 Tokens per Sec: 102.302490\n",
            "Epoch Step: 1 Loss: 1.181730 Tokens per Sec: 102.812401\n",
            "Epoch Step: 51 Loss: 1.235219 Tokens per Sec: 100.386368\n",
            "Epoch Step: 101 Loss: 1.113597 Tokens per Sec: 101.136169\n",
            "Epoch Step: 151 Loss: 1.033791 Tokens per Sec: 102.151299\n",
            "Epoch Step: 201 Loss: 0.979441 Tokens per Sec: 100.703178\n",
            "Epoch Step: 1 Loss: 0.882693 Tokens per Sec: 102.712067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6UrxlsibkvR"
      },
      "source": [
        "**Saving the trained model (and vocab):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvKs--bxaMPF"
      },
      "source": [
        "import pickle \n",
        "\n",
        "with open(\"vocab/arxiv_system_titles_10000.pkl\", 'wb') as outp:\n",
        "  pickle.dump(voc, outp, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "torch.save(model.state_dict(), \"models/titles_system_std_mask_10000_30epochs.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50JasOtsbpnQ"
      },
      "source": [
        "Loading model (and vocab):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLC1pB8Kbq9N",
        "outputId": "f47e3264-5aa6-4198-e45a-0fb963a3a52f"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"vocab/arxiv_system_titles_10000.pkl\", 'rb') as inp:\n",
        "    voc = pickle.load(inp)\n",
        "\n",
        "V = voc.num_words\n",
        "model = make_model(V, N=12)\n",
        "model.load_state_dict(torch.load(\"models/titles_system_std_mask_10000_30epochs.pt\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twsBQ3aQ9Bnq",
        "outputId": "c0317bf6-486a-46e0-8a05-f96bef3338c8"
      },
      "source": [
        "print(V)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvbstYexxFt8"
      },
      "source": [
        "Testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNF-DXeGeEQK"
      },
      "source": [
        "def greedy_decode(model, max_len, symbol_list):\n",
        "\tys = torch.Tensor([symbol_list]).long()\n",
        "\tfor i in range(max_len-1):\n",
        "\t\tout = model.forward(ys, subsequent_mask(ys.size(1)))\n",
        "\t\tprob = model.generator(out[:, -1])\n",
        "\t\t_, next_word = torch.max(prob, dim = 1)\n",
        "\t\tnext_word = next_word.data[0]\n",
        "\t\tys = torch.cat([ys, \n",
        "\t\t\t\t\t\ttorch.ones(1, 1).long().fill_(next_word)], dim=1)\n",
        "\t\tif next_word == voc.to_index('<EOS>'):\n",
        "\t\t\t\tbreak\n",
        "\tprint([voc.to_word(index.item()) for index in ys[0]])\n",
        "\treturn ys"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR4dN4OI6LTb",
        "outputId": "b832a3c5-8938-4934-f755-5f8a20f6a7c8"
      },
      "source": [
        "model.eval()\n",
        "sentence_start = ['<BOS>', 'an' , 'understanding']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS>', 'an', 'understanding', '-', 'complex', '-', 'the', 'complexity', 'of', 'the', 'immune', 'system', ':', 'a', 'complex', '-', 'theoretic', 'approach', 'to', 'parametric', 'complexity', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2PH_DGb8chy",
        "outputId": "bf5c0f46-54ce-4c1c-f51e-18072e6f7e55"
      },
      "source": [
        "sentence_start = ['<BOS>', 'fluctuations' , 'of']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS>', 'fluctuations', 'of', 'quantum', 'statistical', 'two', '-', 'dimensional', 'systems', 'of', 'electrons', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ji51ZHN8iju",
        "outputId": "a6e64a25-6b7a-4ec5-8380-41d65e90e526"
      },
      "source": [
        "sentence_start = ['<BOS>','understanding']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS>', 'understanding', 'the', 'influence', 'of', 'individual', \"'s\", 'self', '-', 'efficacy', 'for', 'information', 'systems', 'security', 'innovation', 'adoption', ':', 'a', 'systematic', 'literature', 'review', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMmotX608BkD",
        "outputId": "9f1c215b-b389-4bec-b4ce-5d3f3182305e"
      },
      "source": [
        "sentence_start = ['<BOS>', 'the' , 'mechanism']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS>', 'the', 'mechanism', 'of', 'scale', '-', 'invariance', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOqPIb7P8Bvx",
        "outputId": "3f0964c8-d0c6-48af-b614-71a59272e3e3"
      },
      "source": [
        "sentence_start = ['<BOS>', 'the' , 'analog']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS>', 'the', 'analog', 'of', 'the', 't', 'system', ':', 'a', 'massive', 'component', 'sharing', 'for', 'the', 'voxceleb', 'learning', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShtlJstSz0K2",
        "outputId": "023a102e-f594-423f-f946-cb2534551ca9"
      },
      "source": [
        "sentence_start = ['<BOS>', 'open']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)\n",
        "sentence_start = ['<BOS>', '$']\n",
        "symbol_list = [voc.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS>', 'open', 'quantum', 'system', 'identification', '<EOS>']\n",
            "['<BOS>', '$', '\\\\text{h}_{\\\\infty}$', 'tracking', 'control', 'via', 'variable', 'gain', 'gradient', 'descent', '-', 'based', 'integral', 'reinforcement', 'learning', 'for', 'unknown', 'continuous', 'time', 'nonlinear', 'system', '<EOS>']\n"
          ]
        }
      ]
    }
  ]
}