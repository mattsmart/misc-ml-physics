{"cells":[{"cell_type":"code","execution_count":1,"id":"c18ec27a","metadata":{"id":"c18ec27a","executionInfo":{"status":"ok","timestamp":1640380923051,"user_tz":300,"elapsed":385,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"id":"MZmtW9scbJ9Y","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43152,"status":"ok","timestamp":1640380966612,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"MZmtW9scbJ9Y","outputId":"31d935b7-7383-4bf9-f63b-c2140c042070"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Github/Abstract-generator/notebooks\n"]}],"source":["IN_COLAB = 'google.colab' in str(get_ipython())\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    %cd /content/drive/MyDrive/Github/Abstract-generator/notebooks"]},{"cell_type":"code","execution_count":3,"id":"2p8t9jjUlx_O","metadata":{"executionInfo":{"elapsed":18112,"status":"ok","timestamp":1640380984696,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"2p8t9jjUlx_O"},"outputs":[],"source":["%%capture\n","if IN_COLAB:\n","  !pip install feedparser tokenizers transformers scipy==1.7.1;"]},{"cell_type":"code","execution_count":4,"id":"04f6ae9e","metadata":{"executionInfo":{"elapsed":17436,"status":"ok","timestamp":1640381002123,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"04f6ae9e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ca5af23-e149-4bf9-c126-e627f3d95d9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Abstract-generator\n"]}],"source":["import os, torch, time, math, sys, re, csv\n","import numpy as np\n","\n","PACKAGE_ROOT = os.path.dirname(os.path.abspath(''))\n","print(PACKAGE_ROOT)\n","sys.path.append(PACKAGE_ROOT)\n","\n","from src import settings\n","import src.data.dataset_class as dsc\n","import src.data.dataloader_class as dlc\n","\n","from src.model.transformer_torch import TransformerModel\n","from src.model.generate_text import gen_some_text, decode_during_training\n","\n","from src.model.train_evaluate import train_version_jeremy as train\n","from src.model.train_evaluate import evaluate_version_jeremy as evaluate\n","\n","#from src.model.transformer import make_gpt_model # imports don't work\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","id":"904013cd","metadata":{"id":"904013cd"},"source":["### Parameters"]},{"cell_type":"code","execution_count":16,"id":"da73442b","metadata":{"executionInfo":{"elapsed":409,"status":"ok","timestamp":1640381360716,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"da73442b"},"outputs":[],"source":["# ARCHITECTURE\n","# TODO : Make a class that sets all this, validates it's in use\n","max_len_sentence     = 40 # maximum sentence length\n","vocab_size  = None # None if you want to let tokenizer do its thing\n","emsize     = 512 # embedding dimension\n","nhid       = 2048 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n","nlayers    = 12 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n","nhead      = 8 # the number of heads in the multiheadattention models\n","dropout    = 0.2 # the dropout value\n","batch_size = 10 #32\n","val_batch_size = 10 #32, not used right now.\n","epochs     = 50  # The number of epochs\n","\n","#train tokenizer (or use one already trained)\n","tknzr_type = 'BPE'\n","flag_tknzr_train = True\n","flag_tknzr_fast = True\n","\n","TRAIN = True"]},{"cell_type":"markdown","id":"21983a34","metadata":{"id":"21983a34"},"source":["### Format Dataset\n","\n","Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."]},{"cell_type":"code","execution_count":17,"id":"015ae225","metadata":{"executionInfo":{"elapsed":9178,"status":"ok","timestamp":1640381370187,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"015ae225","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf96c76f-9611-4951-82d4-c423836b501b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning : overwriting previously save tokenizer with                        same filename ( /content/drive/MyDrive/Github/Abstract-generator/saved_tokenizers/BPE_arxiv_all_electron_10000.json ).\n"]}],"source":["# create dataset\n","dataset = dsc.ArxivDataset()\n","#dataset = dsc.WikiTextDataset()\n","\n","_ = dataset.tokenizer(flag_tknzr_train, tknzr_type, flag_tknzr_fast=flag_tknzr_fast)"]},{"cell_type":"markdown","id":"4bba4cb9","metadata":{"id":"4bba4cb9"},"source":["### Creating DataLoaders\n","\n","Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n","Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n","The collation of batches for different transformer models we have vary. For HuggingFace it's ( max_len x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x max_len ).\n","\n","I created a custom Dataloader class that wraps splitting the dataset and also outputs different dataloaders for each.\n","\n","NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n","FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n","https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"]},{"cell_type":"code","execution_count":18,"id":"c11048fc","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1640381370189,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"c11048fc"},"outputs":[],"source":["flag_padding_mask = True\n","\n","dataloader = dlc.CustomDataloader(dataset, batch_size, max_len_sentence, flag_padding_mask=flag_padding_mask) "]},{"cell_type":"markdown","id":"e3955854","metadata":{"id":"e3955854"},"source":["### Selecting model\n","\n","Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "]},{"cell_type":"code","execution_count":19,"id":"6de7d252","metadata":{"executionInfo":{"elapsed":1014,"status":"ok","timestamp":1640381371190,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"6de7d252"},"outputs":[],"source":["# transformer from huggingface\n","# TODO : Change to the Annotated Transformer if I want\n","model = TransformerModel(dataset.vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n","\n","# criterion\n","criterion = torch.nn.CrossEntropyLoss()#ignore_index=tknzr.get_vocab()[\"<pad>\"])\n","\n","# optimizer TODO : why these parameters?\n","paramsAdam  = [{'params' : model.parameters(), 'lr' : 1e-3, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n","paramsAdamW = [{'params' : model.parameters(), 'lr' : 5e-5, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n","paramsSGD   = [{'params' : model.parameters(), 'lr' : 0.5, 'momentum' : 0.0, 'dampening' : 0.0, 'weight_decay' : 0.0}]\n","\n","optimizer = torch.optim.SGD( paramsSGD )\n","#optimizer = torch.optim.Adam( paramsAdam )\n","#optimizer = torch.optim.AdamW( paramsAdamW )\n","\n","# scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # 1.0 to signify no decay rate"]},{"cell_type":"code","source":["gen_text_each_epoch = True\n","# default behaviour generate sentence 1 greedily, then nseed sentences with beta=1\n","if gen_text_each_epoch:\n","    text_prompt = 'The dog ran'\n","    decode_seeds = [0, 1, 2]\n","    decode_betas = [1.0, 1.0, 1.0]\n","    nongreedy_style = 'sample_full'"],"metadata":{"id":"xNEXSEoMb-9-","executionInfo":{"status":"ok","timestamp":1640381371198,"user_tz":300,"elapsed":35,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"id":"xNEXSEoMb-9-","execution_count":20,"outputs":[]},{"cell_type":"markdown","id":"9780d86f","metadata":{"id":"9780d86f"},"source":["### Training\n","\n","Training loop!"]},{"cell_type":"code","execution_count":null,"id":"75775f0e","metadata":{"id":"75775f0e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"accd075c-485e-483d-dfcb-88220b6d1cd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   200/  700 batches | lr 0.50 | ms/batch 150.96 | loss  7.83 | ppl  2518.93\n"]}],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # fasttokenizer should not be used before forking. Something\n","                                                # to figure out. What this does is suppress some warning messages \n","                                                # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n","                                                # doesn't seem to affect the timing though\n","\n","if TRAIN:\n","    nbr_batches = len(dataloader.train)\n","    training_ppl_train_fine = np.zeros(epochs * nbr_batches)\n","    training_ppl_val_coarse = np.zeros(epochs)\n","    best_val_loss = float(\"inf\")\n","    best_model = None\n","    for epoch in range(1, epochs + 1):\n","        epoch_start_time = time.time()\n","        loss_per_batch = train(model, dataloader.train, device, dataset.vocab_size, epoch, optimizer, scheduler, criterion, max_len_sentence)\n","        val_loss = evaluate(model, dataloader.valid, device, dataset.vocab_size, criterion, max_len_sentence)\n","        validation_ppl = math.exp(val_loss)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                         val_loss, validation_ppl))\n","        print('-' * 89)\n","        \n","        training_ppl_val_coarse[epoch - 1] = validation_ppl\n","        training_ppl_train_fine[(epoch - 1)*nbr_batches: (epoch)*nbr_batches] = np.exp(loss_per_batch)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model\n","\n","        scheduler.step()\n","\n","        if gen_text_each_epoch:\n","            decode_during_training(model, dataset.transform, device, epoch,\n","                                   nongreedy_style,\n","                                   max_len_sentence,\n","                                   text_prompt,\n","                                   decode_seeds,\n","                                   decode_betas)\n","\n","    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_train_fine.txt', training_ppl_train_fine)\n","    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_val_coarse.txt', training_ppl_val_coarse)\n","\n","    # save best model (two methods)\n","    model_full = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}.pth'\n","    model_weights = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}.pth'\n","    model_full_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_best.pth'\n","    model_weights_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}_best.pth'\n","    # approach 1: save model (class) entirely (uses pickle)\n","    torch.save(model, model_full)\n","    torch.save(best_model, model_full_best)\n","    # approach 2: save model weights\n","    torch.save(best_model.state_dict(), model_weights_best)"]},{"cell_type":"markdown","source":["## Plotting Loss"],"metadata":{"id":"fhFjHInPiK_A"},"id":"fhFjHInPiK_A"},{"cell_type":"code","execution_count":null,"id":"NDDjgoQqjODa","metadata":{"id":"NDDjgoQqjODa"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","epochs_axis = np.arange(1, epochs + 1)\n","epochs_axis_fine = np.linspace(0, epochs, epochs * nbr_batches + 1)[:-1]\n","\n","def plot_training_timeseries(exp_name='', fname='training_performance',\n","                             ext='.jpg',\n","                             logy=True, \n","                             xlims=None, \n","                             ylims=None\n","                             ):\n","\n","    plt.figure(figsize=(8,6))\n","    plt.plot(epochs_axis, training_ppl_val_coarse, '--ok', label='val', zorder=2)\n","    plt.plot(epochs_axis_fine, training_ppl_train_fine, 'b', label='train', zorder=1, alpha=0.5)\n","    plt.xlabel('epoch')\n","    plt.ylabel('ppl (validation set)')\n","    plt.legend()\n","    if logy:\n","        plt.yscale('log')\n","        fname += '_logy'\n","    if xlims is not None:\n","        plt.xlim(xlims[0], xlims[1])\n","    if ylims is not None:\n","        plt.ylim(ylims[0], ylims[1])\n","    plt.savefig(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_{fname} + ext')\n","    plt.show()"]},{"cell_type":"code","source":["plot_training_timeseries(exp_name=dataset.name, logy=True)\n","plot_training_timeseries(exp_name=dataset.name, fname='training_performance_linear', logy=False, ylims=(0, np.max(training_ppl_val_coarse) * 1.2))"],"metadata":{"id":"CPgCoA_uigGn"},"id":"CPgCoA_uigGn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"XM5G4vD0q9TZ","metadata":{"id":"XM5G4vD0q9TZ"},"source":["### Text Generation\n","\n","Here I've simply taken the code Matt uses to generate text."]},{"cell_type":"code","execution_count":null,"id":"eb6acee1","metadata":{"id":"eb6acee1"},"outputs":[],"source":["if not TRAIN:\n","    custom_filename = 'arxiv_10000'\n","    custom_epochs = 10\n","    model_full = settings.DIR_MODELS + os.sep + f'{custom_filename}_epoch_{custom_epochs}_best.pth'\n","    model_weights = settings.DIR_MODELS + os.sep + f'{custom_filename}_weights_epoch_{custom_epochs}_best.pth'\n","    \n","    # approach 1: load model (class) entirely (uses pickle)\n","    model_full_load = torch.load(model_full, map_location=device)\n","\n","    # approach 2: load model weights, need to have some parameter or something \n","    model_load = TransformerModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n","    model_weights_load = model_load.load_state_dict( torch.load(model_weights) )"]},{"cell_type":"code","execution_count":null,"id":"SoGylu7hq7kI","metadata":{"id":"SoGylu7hq7kI"},"outputs":[],"source":["# inspect both models\n","#print('model_A info...\\n', model_full_load)\n","#print('\\nmodel_B info...\\n', model_weights_load)\n","\n","#print('model_A == model_B:', model_full_load == model_weights_load)\n","#model = model_full_load\n","# Text generation example\n","\n","#model = model_load\n","prompt = 'The dog ran'\n","ngen = 100\n","decode_style = 'sample_topp' #greedy, sample_topp\n","model.to('cpu')\n","generated_text = gen_some_text(\n","    best_model, dataset.transform, 'cpu', max_len_sentence, text_prompt=prompt, tokens_to_gen=ngen, vis=False,\n","    decode_style=decode_style)\n","print(\"Text prompt:\\n\", prompt)\n","print(\"Number of tokens to generate:\", ngen)\n","print(\"Generated_text:\\n\", generated_text)\n","\n","# TODO: alternative generation\n","# currently 'greedy method'\n","# see: https://huggingface.co/blog/how-to-generate"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"training_and_evaluation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":5}