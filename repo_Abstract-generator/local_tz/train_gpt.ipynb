{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_gpt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylQ1Xscs60t"
      },
      "source": [
        "Mostly re-using Nava's code to\n",
        "\n",
        "1.   Download data from arxiv\n",
        "2.   Tokenize using spacy\n",
        "3.   Build data batches using pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zERKMV41uCZ_"
      },
      "source": [
        "**Downloading data from arxiv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYWAv6uyt_cL",
        "outputId": "733968d8-1e9b-4d92-df88-247c5b444efe"
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import urllib.request\n",
        "import feedparser\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/15bf6781a861bbc5dd801d467f26448fb322bfedcd30f2e62b148d104dfb/feedparser-6.0.8-py3-none-any.whl (81kB)\n",
            "\r\u001b[K     |████                            | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=f7933ee2de54a3d2e42f4f8a60ddaa6b6767ec11fcd39dc3ed754613a5527298\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.8 sgmllib3k-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojpWC6KVuI26"
      },
      "source": [
        "# Base api query url\n",
        "base_url = 'http://export.arxiv.org/api/query?';\n",
        "\n",
        "# Search parameters\n",
        "search_query = 'all:electron' # search for electron in all fields\n",
        "start = 0                     # retreive the first 5 results\n",
        "max_results = 10**3\n",
        "\n",
        "query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
        "                                                     start,\n",
        "                                                     max_results)\n",
        "\n",
        "# perform a GET request using the base_url and query\n",
        "response = urllib.request.urlopen(base_url+query).read()\n",
        "\n",
        "# parse the response using feedparser\n",
        "feed = feedparser.parse(response)\n",
        "\n",
        "#columns of interest\n",
        "col=['title', 'summary', 'authors', 'arxiv_primary_category', 'tags']\n",
        "\n",
        "# Run through each entry, and fill the information into a list\n",
        "data_list=[]\n",
        "for c in col:\n",
        "\tabstract_list=[]\n",
        "\tfor entry in feed.entries:\n",
        "\t\tabstract_list.append(entry.get(c))\n",
        "\tdata_list.append(abstract_list)\n",
        " \n",
        " # convert into a panda dataframe \n",
        "data_df = pd.DataFrame(data_list,index=col)\n",
        "data_df=data_df.T"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2LPRv-as4rY"
      },
      "source": [
        "**Tokenize using spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5LM2NIzu--9",
        "outputId": "07127b94-d8a3-4293-9f19-c6725e42ef2c"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180945 sha256=274b5d5bfdc4192420a97a4c07f222024658f60709271fbda4845fe0696e5813\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-shnk_5wo/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcMSwpTv427"
      },
      "source": [
        "#taking titles\n",
        "# using lower case. removing extra spaces and '\\n ' \n",
        "doc=[nlp.tokenizer(text.lower().replace('\\n ','').strip()) for text in data_list[0]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCRz5TWfwlWW"
      },
      "source": [
        "## manually constructing vocabulary\n",
        "class Vocabulary:\n",
        "    PAD_token = 0   # Used for padding short sentences\n",
        "    BOS_token = 1   # Beginning-of-sentence token\n",
        "    EOS_token = 2   # End-of-sentence token\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.num_words = 0\n",
        "        self.num_sentences = 0\n",
        "        self.longest_sentence = 0\n",
        "\n",
        "        ## add PAD, BOS, EOS tokens:\n",
        "        self.word2index['<PAD>'] = self.num_words\n",
        "        self.word2count['<PAD>'] = 1\n",
        "        self.index2word[self.num_words] = '<PAD>'\n",
        "        self.num_words += 1\n",
        "\n",
        "        self.word2index['<BOS>'] = self.num_words\n",
        "        self.word2count['<BOS>'] = 1\n",
        "        self.index2word[self.num_words] = '<BOS>'\n",
        "        self.num_words += 1\n",
        "\n",
        "        self.word2index['<EOS>'] = self.num_words\n",
        "        self.word2count['<EOS>'] = 1\n",
        "        self.index2word[self.num_words] = '<EOS>'\n",
        "        self.num_words += 1\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # First entry of word into vocabulary\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # Word exists; increase word count\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 1 # length of sentence + <EOS> or <BOS>\n",
        "        for word in [token.text for token in sentence]:\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            # This is the longest sentence\n",
        "            self.longest_sentence = sentence_len\n",
        "        # Count the number of sentences\n",
        "        self.num_sentences += 1\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jHu4X-pwpSI"
      },
      "source": [
        "voc=Vocabulary('abstracts')\n",
        "for sent in doc:\n",
        " \tvoc.add_sentence(sent)\n",
        "  \n",
        "Input_list=[]\n",
        "for sample in range(len(doc)):\n",
        "\tInput_list.append([voc.to_index(\"<BOS>\")]+[voc.to_index(token.text) for token in doc[sample]]+[voc.to_index(\"<EOS>\")])\n",
        "Output_list=Input_list;\n",
        "Input_Output_Data_list=[Input_list,Output_list]\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zy5ltFexHkr"
      },
      "source": [
        "**Building datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY7t8lC4xMgB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#10% test set\n",
        "In_train, In_test, Out_train, Out_test = train_test_split(Input_list, Output_list, test_size=0.1, random_state=1)\n",
        "\n",
        "#from 90% train set --> 20% validation and 80 % training (= in total we have 10% test, 18% val, 72% train )\n",
        "In_train, In_val, Out_train, Out_val = train_test_split(In_train, Out_train , test_size=0.2, random_state=1)\n",
        "\n",
        "train_list=  In_train\n",
        "label=Out_train\n",
        "validation_list=In_val\n",
        "test_list=In_test"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHsgmwyp3ieL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa623da-e78b-49ba-948f-d0fd40c79063"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for _sample in batch:\n",
        "        label_list.append(torch.tensor(_sample[:-1])) # shift stuff here\n",
        "        text_list.append(torch.tensor(_sample[1:]))\n",
        "    return pad_sequence(label_list, padding_value=0.0), pad_sequence(text_list, padding_value=0.0)\n",
        "\n",
        "batch_size = 30\n",
        "\n",
        "def create_iterators(batch_size=batch_size):\n",
        "    \"\"\"Heler function to create the iterators\"\"\"\n",
        "    dataloaders = []\n",
        "    for split in [train_list, validation_list, test_list]:\n",
        "        dataloader = DataLoader(\n",
        "            split, batch_size=batch_size,\n",
        "            collate_fn=collate_batch\n",
        "            )\n",
        "        dataloaders.append(dataloader)\n",
        "    return dataloaders\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = create_iterators()\n",
        "for i, batch in enumerate(train_iterator):\n",
        "  if i < 5:\n",
        "    # print(\"0th element: \",batch[0])\n",
        "    # print([index for index in batch[0]])\n",
        "    print(batch[0].shape)\n",
        "    print(\"0th element: \",[voc.to_word(index.item()) for index in batch[0].T[0]])\n",
        "    print(\"1st element: \",[voc.to_word(index.item()) for index in batch[1].T[0]])\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([26, 30])\n",
            "0th element:  ['<BOS>', 'observation', 'of', 'electron', '-', 'hole', 'puddles', 'in', 'graphene', 'using', 'a', 'scanning', 'single', 'electron', 'transistor', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "1st element:  ['observation', 'of', 'electron', '-', 'hole', 'puddles', 'in', 'graphene', 'using', 'a', 'scanning', 'single', 'electron', 'transistor', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "torch.Size([26, 30])\n",
            "0th element:  ['<BOS>', 'escape', 'of', 'trapped', 'electrons', 'from', 'a', 'helium', 'surface', ':', 'a', 'dynamical', 'theory', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "1st element:  ['escape', 'of', 'trapped', 'electrons', 'from', 'a', 'helium', 'surface', ':', 'a', 'dynamical', 'theory', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "torch.Size([26, 30])\n",
            "0th element:  ['<BOS>', 'manifestation', 'of', 'marginal', 'fermi', 'liquid', 'and', 'phonon', 'excitations', 'in', 'photoemision', 'experiments', 'of', 'cuprate', 'superconductors', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "1st element:  ['manifestation', 'of', 'marginal', 'fermi', 'liquid', 'and', 'phonon', 'excitations', 'in', 'photoemision', 'experiments', 'of', 'cuprate', 'superconductors', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "torch.Size([20, 30])\n",
            "0th element:  ['<BOS>', 'metal', 'hydrogen', 'sulfide', 'critical', 'temperature', 'at', 'the', 'pressure', '225', 'gpa', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "1st element:  ['metal', 'hydrogen', 'sulfide', 'critical', 'temperature', 'at', 'the', 'pressure', '225', 'gpa', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "torch.Size([28, 30])\n",
            "0th element:  ['<BOS>', 'coulomb', 'drag', 'in', 'systems', 'with', 'tunneling', 'bridges', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "1st element:  ['coulomb', 'drag', 'in', 'systems', 'with', 'tunneling', 'bridges', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwmmncuZ3-IM"
      },
      "source": [
        "**Make model and train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cz5g2onvyg5"
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from misc_functions import attention, subsequent_mask\n",
        "from gpt_model import *\n",
        "import math, copy, time"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agPcjI_P4IeO"
      },
      "source": [
        "def make_model(vocab, N=12, \n",
        "\t\t\t   d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "\t\"\"\"Helper: Construct a model from hyperparameters.\"\"\"\n",
        "\n",
        "\t## returns EncoderDecoder object\n",
        "\tc = copy.deepcopy\n",
        "\tattn = MultiHeadedAttention(h, d_model)\n",
        "\tff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\tposition = PositionalEncoding(d_model, dropout)\n",
        "\tmodel = GPT(Decoder(DecoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "\t\t## Sequential passes input to the forward() method in the first module it stores\n",
        "\t\t## and then \"chains\" outputs to inputs sequentially for subsequent modules,\n",
        "\t\tnn.Sequential(Embeddings(d_model, vocab), c(position)),\n",
        "\t\tGenerator(d_model, vocab))\n",
        "\t\n",
        "\t# This was important from their code. \n",
        "\t# Initialize parameters with Glorot / fan_avg.\n",
        "\tfor p in model.parameters():\n",
        "\t\tif p.dim() > 1:\n",
        "\t\t\tnn.init.xavier_uniform_(p) # what does this do? How does it modify model?\n",
        "\treturn model"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd6HiX9M4icE"
      },
      "source": [
        "Optimizer, loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApyssFBh4PUT"
      },
      "source": [
        "class NoamOpt:\n",
        "\t#\"Optim wrapper that implements rate.\"\n",
        "\tdef __init__(self, model_size, factor, warmup, optimizer):\n",
        "\t\tself.optimizer = optimizer\n",
        "\t\tself._step = 0\n",
        "\t\tself.warmup = warmup\n",
        "\t\tself.factor = factor\n",
        "\t\tself.model_size = model_size\n",
        "\t\tself._rate = 0\n",
        "\t\t\n",
        "\tdef step(self):\n",
        "\t\t# \"Update parameters and rate\"\n",
        "\t\tself._step += 1\n",
        "\t\trate = self.rate()\n",
        "\t\tfor p in self.optimizer.param_groups:\n",
        "\t\t\tp['lr'] = rate\n",
        "\t\tself._rate = rate\n",
        "\t\tself.optimizer.step()\n",
        "\t\t\n",
        "\tdef rate(self, step = None):\n",
        "\t\t# \"Implement `lrate` above\"\n",
        "\t\tif step is None:\n",
        "\t\t\tstep = self._step\n",
        "\t\treturn self.factor * \\\n",
        "\t\t\t(self.model_size ** (-0.5) *\n",
        "\t\t\tmin(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "\t# \"Implement label smoothing.\"\n",
        "\tdef __init__(self, size, padding_idx, smoothing=0.0):\n",
        "\t\tsuper(LabelSmoothing, self).__init__()\n",
        "\t\tself.criterion = nn.KLDivLoss(size_average=False) # Kullback-Leibler divergence loss\n",
        "\t\tself.padding_idx = padding_idx\n",
        "\t\tself.confidence = 1.0 - smoothing\n",
        "\t\tself.smoothing = smoothing\n",
        "\t\tself.size = size\n",
        "\t\tself.true_dist = None\n",
        "\t\t\n",
        "\tdef forward(self, x, target):\n",
        "\t\tassert x.size(1) == self.size\n",
        "\t\ttrue_dist = x.data.clone()\n",
        "\t\ttrue_dist.fill_(self.smoothing / (self.size - 2))\n",
        "\t\ttrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "\t\ttrue_dist[:, self.padding_idx] = 0\n",
        "\t\tmask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "\t\tif mask.dim() > 0:\n",
        "\t\t\ttrue_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\t\tself.true_dist = true_dist.requires_grad_(False)\n",
        "\t\t# return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\t\treturn self.criterion(x, true_dist)\n",
        "  \n",
        "  \n",
        "class SimpleLossCompute:\n",
        "\t# \"A simple loss compute and train function.\"\n",
        "\tdef __init__(self, generator, criterion, opt=None):\n",
        "\t\tself.generator = generator\n",
        "\t\tself.criterion = criterion # LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "\t\tself.opt = opt # NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\t\t\n",
        "\tdef __call__(self, x, y, norm):\n",
        "\t\tx = self.generator(x) # x is output, each element now in d_vocab dimensions, shape = [30, 9, 11]\n",
        "\t\t\t\t\t\t\t  # y is batch.trg_y (first column of 1s removed), shape = [30, 9]\n",
        "\t\t\t\t\t\t\t  # norm is batch.ntokens (270)\n",
        "\n",
        "\t\tloss = self.criterion(x.contiguous().view(-1, x.size(-1)), # shape = [270, 11]\n",
        "\t\t\t\t\t\t\t  y.contiguous().view(-1)) / norm # shape = [270]\n",
        "\t\tloss.backward() # compute gradients (of what?)\n",
        "\t\tif self.opt is not None:\n",
        "\t\t\tself.opt.step()\n",
        "\t\t\tself.opt.optimizer.zero_grad()\n",
        "\n",
        "\t\tif list(loss.data.size()) != []:\n",
        "\t\t\treturn loss.data[0] * norm\n",
        "\t\telse:\n",
        "\t\t\treturn loss.data * norm\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U37LcJ1F4ovG"
      },
      "source": [
        "Make model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXdjCgSc4r5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ce459e-93d3-4d2a-d81b-f41bba223578"
      },
      "source": [
        "device = 'gpu'\n",
        "V = voc.num_words\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, N=12).to(device)\n",
        "## uses pytorch's Adam optimizer\n",
        "model_opt = NoamOpt(model.embed[0].d_model, 1, 400,\n",
        "\t\ttorch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWeHuwvo5TAO"
      },
      "source": [
        "train_data=In_train\n",
        "\n",
        "def run_epoch(model, loss_compute):\n",
        "\t\"\"\"Standard Training and Logging Function\"\"\"\n",
        "\tstart = time.time()\n",
        "\ttotal_tokens = 0\n",
        "\ttotal_loss = 0\n",
        "\ttokens = 0\n",
        "\ttrain_iterator, valid_iterator, test_iterator = create_iterators()\n",
        "\tfor batch, i in enumerate(range(0, len(train_data) - 1)):\n",
        "\t\tdata, targets=next(iter(train_iterator)) # change both data and target\n",
        "\t\tmask = subsequent_mask(data.size(0)).to(device)\n",
        "\t\tout = model.forward(data.T, mask)\n",
        "\t\tloss = loss_compute(out, targets.T, V)\n",
        "\t\ttotal_loss += loss\n",
        "\t\ttotal_tokens += V\n",
        "\t\ttokens += V\n",
        "\t\tif i % 50 == 1:\n",
        "\t\t\telapsed = time.time() - start\n",
        "\t\t\tprint(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "\t\t\t\t\t(i, loss / V, tokens / elapsed))\n",
        "\t\t\tstart = time.time()\n",
        "\t\t\ttokens = 0\n",
        "\treturn total_loss / total_tokens"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQsug6PnUMLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c06bc3a-ae41-4771-af93-9a27e7bbbfa7"
      },
      "source": [
        "\n",
        "model.train() ## calls nn.Module.train() which sets mode to train\n",
        "run_epoch(model, # generates 20 batches of [30, 10] random integers (first column is 1)\n",
        "      SimpleLossCompute(model.generator, criterion, model_opt))\n",
        "# model.eval() ## sets mode to testing (i.e. train=False). Layers like dropout behave differently depending on if mode is train or testing.\n",
        "# run_epoch(model, \n",
        "#         SimpleLossCompute(model.generator, criterion, None))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 1 Loss: 1.622465 Tokens per Sec: 368.875506\n",
            "Epoch Step: 51 Loss: 0.201556 Tokens per Sec: 488.866957\n",
            "Epoch Step: 101 Loss: 0.051840 Tokens per Sec: 489.427552\n",
            "Epoch Step: 151 Loss: 0.054618 Tokens per Sec: 534.214603\n",
            "Epoch Step: 201 Loss: 0.051482 Tokens per Sec: 534.717503\n",
            "Epoch Step: 251 Loss: 0.050731 Tokens per Sec: 527.073128\n",
            "Epoch Step: 301 Loss: 0.050033 Tokens per Sec: 536.063833\n",
            "Epoch Step: 351 Loss: 0.050814 Tokens per Sec: 536.271535\n",
            "Epoch Step: 401 Loss: 0.049945 Tokens per Sec: 531.607598\n",
            "Epoch Step: 451 Loss: 0.052882 Tokens per Sec: 525.717861\n",
            "Epoch Step: 501 Loss: 0.056537 Tokens per Sec: 514.674348\n",
            "Epoch Step: 551 Loss: 0.051117 Tokens per Sec: 508.770060\n",
            "Epoch Step: 601 Loss: 0.052809 Tokens per Sec: 488.726225\n",
            "Epoch Step: 651 Loss: 0.055966 Tokens per Sec: 447.902482\n",
            "Epoch Step: 701 Loss: 0.051520 Tokens per Sec: 410.974630\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1246)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNF-DXeGeEQK"
      },
      "source": [
        "def greedy_decode(model, max_len, start_symbol):\n",
        "\tys = torch.ones(1, 1).fill_(start_symbol).long()\n",
        "\tfor i in range(max_len-1):\n",
        "\t\tout = model.forward(ys, subsequent_mask(ys.size(1)))\n",
        "\t\tprob = model.generator(out[:, -1])\n",
        "\t\t_, next_word = torch.max(prob, dim = 1)\n",
        "\t\t# print(next_word)\n",
        "\t\tnext_word = next_word.data[0]\n",
        "\t\t# print(voc.to_word(next_word.item()))\n",
        "\t\tys = torch.cat([ys, \n",
        "\t\t\t\t\t\ttorch.ones(1, 1).long().fill_(next_word)], dim=1)\n",
        "\t# print(ys)\n",
        "\tprint([voc.to_word(index.item()) for index in ys[0]])\n",
        "\treturn ys"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAYhw5bQUXod",
        "outputId": "7c3d7b80-ba23-4ead-d63a-3bf1b67f95e0"
      },
      "source": [
        "greedy_decode(model, 20, 1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<BOS>', 'charge', 'occupancy', 'of', 'two', 'interacting', 'electrons', 'on', 'artificial', 'molecules', '-', 'exact', 'results', '<EOS>', 'results', '<EOS>', 'ion', 'interaction', 'and', 'in']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   1,  429, 1138,    4,   44,  486,   19,    8, 1139,  630,    6,   57,\n",
              "          558,    2,  558,    2,  153,   10,   28,   33]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3imE73bFoIvB",
        "outputId": "ee1eba4d-b74c-49d0-fe05-c422dc9eb16e"
      },
      "source": [
        "greedy_decode(model, 20, 6)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-', 'singular', 'spectral', 'features', 'for', 'the', 'interacting', 'vortex', 'structures', 'in', 'thermostat', 'regime', '<EOS>', 'dimensional', 'electron', 'gas', '<EOS>', '<EOS>', 'ion', 'interaction']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   6, 1195,  112, 1196,   98,   34,  486,  260,  142,   33, 1407,  300,\n",
              "            2,   45,    5,  144,    2,    2,  153,   10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    }
  ]
}