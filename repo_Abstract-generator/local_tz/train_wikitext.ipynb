{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_wikitext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg-E2oIMMG1E",
        "outputId": "602ea545-ae15-47db-d9a6-4be5a048dfb5"
      },
      "source": [
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "TRAIN = True\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    %cd /content/drive/MyDrive/Documents/HLML/abstracts/local_tz/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Documents/HLML/abstracts/local_tz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur4w1IpJXwSB",
        "outputId": "b450ccc5-a7be-48a1-e66f-079a2d5e20f3"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=4ff4b59f74660614a84b06d2e907f0bf1c3eb22c6c038888769a40c2423e07b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tmnogkdo/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhDbAEPZX3_S"
      },
      "source": [
        "## manually constructing vocabulary\n",
        "class Vocabulary:\n",
        "    PAD_token = 0   # Used for padding short sentences\n",
        "    BOS_token = 1   # Beginning-of-sentence token\n",
        "    EOS_token = 2   # End-of-sentence token\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.num_words = 0\n",
        "        self.num_sentences = 0\n",
        "        self.longest_sentence = 0\n",
        "\n",
        "        ## add PAD, BOS, EOS tokens:\n",
        "        self.word2index['<PAD>'] = self.num_words\n",
        "        self.word2count['<PAD>'] = 1\n",
        "        self.index2word[self.num_words] = '<PAD>'\n",
        "        self.num_words += 1\n",
        "\n",
        "        self.word2index['<BOS>'] = self.num_words\n",
        "        self.word2count['<BOS>'] = 1\n",
        "        self.index2word[self.num_words] = '<BOS>'\n",
        "        self.num_words += 1\n",
        "\n",
        "        self.word2index['<EOS>'] = self.num_words\n",
        "        self.word2count['<EOS>'] = 1\n",
        "        self.index2word[self.num_words] = '<EOS>'\n",
        "        self.num_words += 1\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # First entry of word into vocabulary\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # Word exists; increase word count\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 0 \n",
        "        for word in [token.text for token in sentence]:\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            self.longest_sentence = sentence_len\n",
        "        self.num_sentences += 1\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao0yMIguX6hc"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiJtj26sYDWE"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for _sample in batch:\n",
        "        label_list.append(torch.tensor(_sample[:-1])) # data\n",
        "        text_list.append(torch.tensor(_sample[1:])) # trg\n",
        "    return pad_sequence(label_list, padding_value=0.0), pad_sequence(text_list, padding_value=0.0)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "def create_iterators(batch_size=batch_size):\n",
        "    \"\"\"Heler function to create the iterators\"\"\"\n",
        "    dataloaders = []\n",
        "    for split in [train_list, validation_list, test_list]:\n",
        "        dataloader = DataLoader(\n",
        "            split, batch_size=batch_size,\n",
        "            collate_fn=collate_batch\n",
        "            )\n",
        "        dataloaders.append(dataloader)\n",
        "    return dataloaders"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__NemmmXYGFH"
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from misc_functions import attention, make_std_mask\n",
        "from gpt_model import *\n",
        "import math, copy, time"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9ZOkAHbYW9t",
        "outputId": "71717fea-5e10-48b3-be03-c591c6d0d40d"
      },
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "train_iter, val_iter, test_iter = WikiText2(split=('train', 'valid', 'test'))\n",
        "\n",
        "Input_list = []\n",
        "\n",
        "voc_w2=Vocabulary('wikitext2')\n",
        "for i, item in enumerate(train_iter):\n",
        "  if len(item) > 2: ## ignore the newlines\n",
        "    sent = nlp.tokenizer(item.lower().strip())\n",
        "    voc_w2.add_sentence(sent)\n",
        "    Input_list.append([voc_w2.to_index(\"<BOS>\")]+[voc_w2.to_index(token.text) for token in sent[:35]]+[voc_w2.to_index(\"<EOS>\")])\n",
        "\n",
        "for i, item in enumerate(val_iter):\n",
        "  if len(item) > 2: ## ignore the newlines\n",
        "    sent = nlp.tokenizer(item.lower().strip())\n",
        "    voc_w2.add_sentence(sent)\n",
        "    Input_list.append([voc_w2.to_index(\"<BOS>\")]+[voc_w2.to_index(token.text) for token in sent[:35]]+[voc_w2.to_index(\"<EOS>\")])\n",
        "\n",
        "for i, item in enumerate(test_iter):\n",
        "  if len(item) > 2: ## ignore the newlines\n",
        "    sent = nlp.tokenizer(item.lower().strip())\n",
        "    voc_w2.add_sentence(sent)\n",
        "    Input_list.append([voc_w2.to_index(\"<BOS>\")]+[voc_w2.to_index(token.text) for token in sent[:35]]+[voc_w2.to_index(\"<EOS>\")])\n",
        "\n",
        "wikitext2_dataset = CustomTextDataset(Input_list)\n",
        "data_len = len(Input_list)\n",
        "print(data_len)\n",
        "train_list, validation_list, test_list = random_split(wikitext2_dataset, [int(data_len*0.72), int(data_len*0.1), data_len-(int(data_len*0.72)+int(data_len*0.1))], generator=torch.Generator().manual_seed(42))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 6.86MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-5uPsCWYIoo"
      },
      "source": [
        "def make_model(vocab, N=12, \n",
        "\t\t\t   d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "\t\"\"\"Helper: Construct a model from hyperparameters.\"\"\"\n",
        "\n",
        "\t## returns EncoderDecoder object\n",
        "\tc = copy.deepcopy\n",
        "\tattn = MultiHeadedAttention(h, d_model)\n",
        "\tff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\tposition = PositionalEncoding(d_model, dropout)\n",
        "\tmodel = GPT(Decoder(DecoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "\t\t## Sequential passes input to the forward() method in the first module it stores\n",
        "\t\t## and then \"chains\" outputs to inputs sequentially for subsequent modules,\n",
        "\t\tnn.Sequential(Embeddings(d_model, vocab), c(position)),\n",
        "\t\tGenerator(d_model, vocab))\n",
        "\t\n",
        "\t# This was important from their code. \n",
        "\t# Initialize parameters with Glorot / fan_avg.\n",
        "\tfor p in model.parameters():\n",
        "\t\tif p.dim() > 1:\n",
        "\t\t\tnn.init.xavier_uniform_(p) # what does this do? How does it modify model?\n",
        "\treturn model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkg5z7BZmlv"
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "\t# \"Implement label smoothing.\"\n",
        "\tdef __init__(self, size, padding_idx, smoothing=0.0):\n",
        "\t\tsuper(LabelSmoothing, self).__init__()\n",
        "\t\tself.criterion = nn.KLDivLoss(size_average=False) # Kullback-Leibler divergence loss\n",
        "\t\tself.padding_idx = padding_idx\n",
        "\t\tself.confidence = 1.0 - smoothing\n",
        "\t\tself.smoothing = smoothing\n",
        "\t\tself.size = size\n",
        "\t\tself.true_dist = None\n",
        "\t\t\n",
        "\tdef forward(self, x, target):\n",
        "\t\tassert x.size(1) == self.size\n",
        "\t\ttrue_dist = x.data.clone()\n",
        "\t\ttrue_dist.fill_(self.smoothing / (self.size - 2))\n",
        "\t\ttrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "\t\ttrue_dist[:, self.padding_idx] = 0\n",
        "\t\tmask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "\t\tif mask.dim() > 0:\n",
        "\t\t\ttrue_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\t\tself.true_dist = true_dist.requires_grad_(False)\n",
        "\n",
        "\t\treturn self.criterion(x, true_dist)\n",
        "  \n",
        "  \n",
        "class SimpleLossCompute:\n",
        "\t# \"A simple loss compute and train function.\"\n",
        "\tdef __init__(self, generator, criterion, opt=None):\n",
        "\t\tself.generator = generator\n",
        "\t\tself.criterion = criterion # LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "\t\tself.opt = opt # NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\t\t\n",
        "\tdef __call__(self, x, y, norm):\n",
        "\t\tx = self.generator(x) # x is output, each element now in d_vocab dimensions, shape = [30, 9, 11]\n",
        "\t\t\t\t\t\t\t  # y is batch.trg_y (first column of 1s removed), shape = [30, 9]\n",
        "\t\t\t\t\t\t\t  # norm is batch.ntokens (270)\n",
        "\t\t\n",
        "\t\tloss = self.criterion(x.contiguous().view(-1, x.size(-1)), # shape = [270, 11]\n",
        "\t\t\t\t\t\t\t  y.contiguous().view(-1)) / norm # shape = [270]\n",
        "\t\t# print(\"Label Smoothing called\")\n",
        "\t\tloss.backward()\n",
        "\t\tif self.opt is not None:\n",
        "\t\t\tself.opt.step()\n",
        "\t\t\tself.opt.zero_grad() # if using default pytorch SGD or Adam\n",
        "\t\t\t# self.opt.optimizer.zero_grad() # if using NoamOpt class\n",
        "\n",
        "\t\tif list(loss.data.size()) != []:\n",
        "\t\t\treturn loss.data[0] * norm\n",
        "\t\telse:\n",
        "\t\t\treturn loss.data * norm\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGuIPxFRZaSC",
        "outputId": "535d5dae-2c9e-4460-d245-2f18497e5815"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
        "print('Device:', device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.is_available(): True\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYlfGC_YZTAT"
      },
      "source": [
        "V = voc_w2.num_words\n",
        "model = make_model(V, N=12).to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0RqcEzQZzxT"
      },
      "source": [
        "def run_epoch(data_iterator, model, loss_compute):\n",
        "\t\"\"\"Standard Training and Logging Function\"\"\"\n",
        "\tstart = time.time()\n",
        "\ttotal_tokens = 0\n",
        "\ttotal_loss = 0\n",
        "\ttokens = 0\n",
        "\n",
        "\tloss_record = []\n",
        " \n",
        "\tfor i, batch in enumerate(data_iterator):\n",
        "\t\tdata = batch[0].T.to(device)\n",
        "\t\ttrg = batch[1].T.to(device)\n",
        "\t\tmask = make_std_mask(trg, pad=0).to(device)\n",
        "\t\tout = model.forward(data, mask)\n",
        "\t\tntokens = (trg != 0).data.sum()\n",
        "\t\tloss = loss_compute(out, trg, ntokens)\n",
        "\t\tloss_record.append(loss)\n",
        "\t\ttotal_loss += loss\n",
        "\t\ttotal_tokens += ntokens \n",
        "\t\ttokens += ntokens\n",
        "\t\tif i % 50 == 1:\n",
        "\t\t\telapsed = time.time() - start\n",
        "\t\t\tprint(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "\t\t\t\t\t(i, loss / ntokens, tokens / elapsed))\n",
        "\t\t\tstart = time.time()\n",
        "\t\t\ttokens = 0\n",
        "\t# return total_loss / total_tokens\n",
        "\t# print(len(loss_record))\n",
        "\treturn np.array(loss_record)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6nNrrwkjBhp"
      },
      "source": [
        "import pickle \n",
        "\n",
        "with open(\"vocab/wikitext2.pkl\", 'wb') as outp:\n",
        "  pickle.dump(voc_w2, outp, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na14rXDrclxA"
      },
      "source": [
        "def greedy_decode(model, max_len, symbol_list):\n",
        "\tys = torch.Tensor([symbol_list]).long().to(device)\n",
        "\tfor i in range(max_len-1):\n",
        "\t\tout = model.forward(ys, subsequent_mask(ys.size(1)).to(device))\n",
        "\t\tprob = model.generator(out[:, -1])\n",
        "\t\t_, next_word = torch.max(prob, dim = 1)\n",
        "\t\tnext_word = next_word.data[0]\n",
        "\t\tys = torch.cat([ys, \n",
        "\t\t\t\t\t\ttorch.ones(1, 1).long().fill_(next_word).to(device)], dim=1)\n",
        "\t\tif next_word == voc_w2.to_index('<EOS>'):\n",
        "\t\t\t\tbreak\n",
        "\tprint([voc_w2.to_word(index.item()) for index in ys[0]])\n",
        "\treturn ys"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5KgZ_G7ZrKy",
        "outputId": "1836b212-7a1e-4fc3-871a-2309a03f3888"
      },
      "source": [
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "\n",
        "## testing with different optimizers\n",
        "model_opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "# model_opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ## uses pytorch's Adam optimizer\n",
        "# model_opt = NoamOpt(model.embed[0].d_model, 1, 4000,\n",
        "# \t\ttorch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = create_iterators()\n",
        "training_loss = np.zeros(0)\n",
        "validation_loss = np.zeros(0)\n",
        "\n",
        "for epoch in range(100):\n",
        "  model.train()\n",
        "  tl = run_epoch(train_iterator, model, SimpleLossCompute(model.generator, criterion, model_opt))\n",
        "  training_loss = np.concatenate([training_loss, tl])\n",
        "  model.eval() \n",
        "  vl = run_epoch(valid_iterator, model, SimpleLossCompute(model.generator, criterion, None))\n",
        "  validation_loss = np.concatenate([validation_loss, vl])\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    torch.save(model.state_dict(), \"models/wikietext2_SGD_{0}epochs.pt\".format(epoch))\n",
        "    sentence_start = ['<BOS>', 'the', 'dog', 'ran']\n",
        "    symbol_list = [voc_w2.to_index(token) for token in sentence_start]\n",
        "    print(greedy_decode(model, 30, symbol_list))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 26.998251 Tokens per Sec: 2961.576660\n",
            "Epoch Step: 51 Loss: 6.078160 Tokens per Sec: 2862.364014\n",
            "Epoch Step: 101 Loss: 5.978872 Tokens per Sec: 2858.023193\n",
            "Epoch Step: 151 Loss: 5.900390 Tokens per Sec: 2870.828125\n",
            "Epoch Step: 201 Loss: 5.913371 Tokens per Sec: 2863.014404\n",
            "Epoch Step: 251 Loss: 5.538423 Tokens per Sec: 2839.856445\n",
            "Epoch Step: 301 Loss: 6.158897 Tokens per Sec: 2889.676758\n",
            "Epoch Step: 1 Loss: 5.876213 Tokens per Sec: 3593.694824\n",
            "['<BOS>', 'the', 'dog', 'ran', ',', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk']\n",
            "tensor([[   1,   21, 5135, 1969,   17,   21,   11,   12,   13,   20,   21,   11,\n",
            "           12,   13,   17,   21,   11,   12,   13,   20,   21,   11,   12,   13,\n",
            "           20,   21,   11,   12,   13,   17,   21,   11,   12]],\n",
            "       device='cuda:0')\n",
            "Epoch Step: 1 Loss: 14.031898 Tokens per Sec: 3243.184326\n",
            "Epoch Step: 51 Loss: 6.115518 Tokens per Sec: 2873.322266\n",
            "Epoch Step: 101 Loss: 5.959409 Tokens per Sec: 2865.239014\n",
            "Epoch Step: 151 Loss: 6.252595 Tokens per Sec: 2874.739746\n",
            "Epoch Step: 201 Loss: 5.957018 Tokens per Sec: 2863.578857\n",
            "Epoch Step: 251 Loss: 5.505589 Tokens per Sec: 2833.832764\n",
            "Epoch Step: 301 Loss: 5.992839 Tokens per Sec: 2890.457275\n",
            "Epoch Step: 1 Loss: 5.915856 Tokens per Sec: 3581.422119\n",
            "Epoch Step: 1 Loss: 19.558693 Tokens per Sec: 2866.445801\n",
            "Epoch Step: 51 Loss: 5.894129 Tokens per Sec: 2877.453125\n",
            "Epoch Step: 101 Loss: 5.966794 Tokens per Sec: 2868.942871\n",
            "Epoch Step: 151 Loss: 6.307695 Tokens per Sec: 2878.313232\n",
            "Epoch Step: 201 Loss: 6.058990 Tokens per Sec: 2866.461426\n",
            "Epoch Step: 251 Loss: 5.670313 Tokens per Sec: 2838.154053\n",
            "Epoch Step: 301 Loss: 6.035318 Tokens per Sec: 2894.124268\n",
            "Epoch Step: 1 Loss: 5.862479 Tokens per Sec: 3611.980225\n",
            "Epoch Step: 1 Loss: 14.105984 Tokens per Sec: 2862.214600\n",
            "Epoch Step: 51 Loss: 5.770612 Tokens per Sec: 2879.426514\n",
            "Epoch Step: 101 Loss: 5.819950 Tokens per Sec: 2867.121826\n",
            "Epoch Step: 151 Loss: 5.889084 Tokens per Sec: 2875.142334\n",
            "Epoch Step: 201 Loss: 5.833807 Tokens per Sec: 2865.535889\n",
            "Epoch Step: 251 Loss: 5.498009 Tokens per Sec: 2835.852539\n",
            "Epoch Step: 301 Loss: 5.953197 Tokens per Sec: 2892.852783\n",
            "Epoch Step: 1 Loss: 5.867840 Tokens per Sec: 3591.211426\n",
            "Epoch Step: 1 Loss: 25.795448 Tokens per Sec: 2864.923096\n",
            "Epoch Step: 51 Loss: 6.665035 Tokens per Sec: 2877.299316\n",
            "Epoch Step: 101 Loss: 6.404664 Tokens per Sec: 2863.424561\n",
            "Epoch Step: 151 Loss: 6.307573 Tokens per Sec: 2880.330078\n",
            "Epoch Step: 201 Loss: 6.105842 Tokens per Sec: 2867.472168\n",
            "Epoch Step: 251 Loss: 5.618308 Tokens per Sec: 2835.492188\n",
            "Epoch Step: 301 Loss: 6.015761 Tokens per Sec: 2898.819580\n",
            "Epoch Step: 1 Loss: 5.929781 Tokens per Sec: 3586.239502\n",
            "Epoch Step: 1 Loss: 18.974457 Tokens per Sec: 2861.502930\n",
            "Epoch Step: 51 Loss: 6.147631 Tokens per Sec: 2873.394775\n",
            "Epoch Step: 101 Loss: 6.007009 Tokens per Sec: 2863.134766\n",
            "Epoch Step: 151 Loss: 5.981084 Tokens per Sec: 2872.584961\n",
            "Epoch Step: 201 Loss: 5.873820 Tokens per Sec: 2861.260742\n",
            "Epoch Step: 251 Loss: 5.546255 Tokens per Sec: 2837.280762\n",
            "Epoch Step: 301 Loss: 6.080294 Tokens per Sec: 2892.115234\n",
            "Epoch Step: 1 Loss: 5.845842 Tokens per Sec: 3627.930664\n",
            "Epoch Step: 1 Loss: 11.774697 Tokens per Sec: 2886.073486\n",
            "Epoch Step: 51 Loss: 5.608071 Tokens per Sec: 2874.332764\n",
            "Epoch Step: 101 Loss: 5.773182 Tokens per Sec: 2868.434326\n",
            "Epoch Step: 151 Loss: 5.791929 Tokens per Sec: 2871.505127\n",
            "Epoch Step: 201 Loss: 5.745777 Tokens per Sec: 2866.809814\n",
            "Epoch Step: 251 Loss: 5.421930 Tokens per Sec: 2837.483398\n",
            "Epoch Step: 301 Loss: 5.926655 Tokens per Sec: 2894.908447\n",
            "Epoch Step: 1 Loss: 5.777208 Tokens per Sec: 3585.826660\n",
            "Epoch Step: 1 Loss: 10.279716 Tokens per Sec: 2875.109131\n",
            "Epoch Step: 51 Loss: 5.619650 Tokens per Sec: 2870.971191\n",
            "Epoch Step: 101 Loss: 5.727786 Tokens per Sec: 2867.525146\n",
            "Epoch Step: 151 Loss: 5.734574 Tokens per Sec: 2869.498535\n",
            "Epoch Step: 201 Loss: 5.723965 Tokens per Sec: 2870.806152\n",
            "Epoch Step: 251 Loss: 5.387963 Tokens per Sec: 2833.841553\n",
            "Epoch Step: 301 Loss: 5.899723 Tokens per Sec: 2894.825195\n",
            "Epoch Step: 1 Loss: 5.743030 Tokens per Sec: 3589.990479\n",
            "Epoch Step: 1 Loss: 9.186869 Tokens per Sec: 2857.650391\n",
            "Epoch Step: 51 Loss: 5.508467 Tokens per Sec: 2872.805908\n",
            "Epoch Step: 101 Loss: 5.680315 Tokens per Sec: 2867.919922\n",
            "Epoch Step: 151 Loss: 5.661769 Tokens per Sec: 2876.265381\n",
            "Epoch Step: 201 Loss: 5.668989 Tokens per Sec: 2868.145996\n",
            "Epoch Step: 251 Loss: 5.320541 Tokens per Sec: 2840.705811\n",
            "Epoch Step: 301 Loss: 5.819580 Tokens per Sec: 2895.356689\n",
            "Epoch Step: 1 Loss: 5.695730 Tokens per Sec: 3583.578857\n",
            "Epoch Step: 1 Loss: 8.700662 Tokens per Sec: 2897.359375\n",
            "Epoch Step: 51 Loss: 5.458818 Tokens per Sec: 2874.770996\n",
            "Epoch Step: 101 Loss: 5.632826 Tokens per Sec: 2869.457275\n",
            "Epoch Step: 151 Loss: 5.619673 Tokens per Sec: 2879.645508\n",
            "Epoch Step: 201 Loss: 5.625535 Tokens per Sec: 2868.434814\n",
            "Epoch Step: 251 Loss: 5.283440 Tokens per Sec: 2841.177734\n",
            "Epoch Step: 301 Loss: 5.790657 Tokens per Sec: 2909.419189\n",
            "Epoch Step: 1 Loss: 5.645521 Tokens per Sec: 3621.612549\n",
            "Epoch Step: 1 Loss: 7.727279 Tokens per Sec: 2926.007568\n",
            "Epoch Step: 51 Loss: 5.426246 Tokens per Sec: 2893.280273\n",
            "Epoch Step: 101 Loss: 5.584911 Tokens per Sec: 2884.038574\n",
            "Epoch Step: 151 Loss: 5.578612 Tokens per Sec: 2892.546875\n",
            "Epoch Step: 201 Loss: 5.584247 Tokens per Sec: 2887.896240\n",
            "Epoch Step: 251 Loss: 5.247666 Tokens per Sec: 2855.199219\n",
            "Epoch Step: 301 Loss: 5.740686 Tokens per Sec: 2914.758301\n",
            "Epoch Step: 1 Loss: 5.605718 Tokens per Sec: 3627.577637\n",
            "['<BOS>', 'the', 'dog', 'ran', 'the', 'first', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk', '>', '<', 'unk', '>', 'of']\n",
            "tensor([[   1,   21, 5135, 1969,   21,   63,   11,   12,   13,   20,   21,   11,\n",
            "           12,   13,   17,   21,   11,   12,   13,   17,   21,   11,   12,   13,\n",
            "           17,   21,   11,   12,   13,   11,   12,   13,   20]],\n",
            "       device='cuda:0')\n",
            "Epoch Step: 1 Loss: 8.668038 Tokens per Sec: 3271.997070\n",
            "Epoch Step: 51 Loss: 5.398009 Tokens per Sec: 2890.377930\n",
            "Epoch Step: 101 Loss: 5.560716 Tokens per Sec: 2885.489014\n",
            "Epoch Step: 151 Loss: 5.549467 Tokens per Sec: 2893.870850\n",
            "Epoch Step: 201 Loss: 5.551281 Tokens per Sec: 2884.481445\n",
            "Epoch Step: 251 Loss: 5.223619 Tokens per Sec: 2858.055908\n",
            "Epoch Step: 301 Loss: 5.730621 Tokens per Sec: 2913.882324\n",
            "Epoch Step: 1 Loss: 5.573468 Tokens per Sec: 3593.543701\n",
            "Epoch Step: 1 Loss: 9.190671 Tokens per Sec: 2875.000000\n",
            "Epoch Step: 51 Loss: 5.359179 Tokens per Sec: 2890.977051\n",
            "Epoch Step: 101 Loss: 5.525250 Tokens per Sec: 2887.133057\n",
            "Epoch Step: 151 Loss: 5.520099 Tokens per Sec: 2893.618896\n",
            "Epoch Step: 201 Loss: 5.516882 Tokens per Sec: 2886.343018\n",
            "Epoch Step: 251 Loss: 5.202913 Tokens per Sec: 2861.197021\n",
            "Epoch Step: 301 Loss: 5.690641 Tokens per Sec: 2898.385742\n",
            "Epoch Step: 1 Loss: 5.530936 Tokens per Sec: 3603.846436\n",
            "Epoch Step: 1 Loss: 8.068103 Tokens per Sec: 2843.302979\n",
            "Epoch Step: 51 Loss: 5.347192 Tokens per Sec: 2870.833496\n",
            "Epoch Step: 101 Loss: 5.486309 Tokens per Sec: 2862.942627\n",
            "Epoch Step: 151 Loss: 5.487056 Tokens per Sec: 2870.731689\n",
            "Epoch Step: 201 Loss: 5.490043 Tokens per Sec: 2863.878418\n",
            "Epoch Step: 251 Loss: 5.184789 Tokens per Sec: 2834.839355\n",
            "Epoch Step: 301 Loss: 5.661110 Tokens per Sec: 2889.144287\n",
            "Epoch Step: 1 Loss: 5.501493 Tokens per Sec: 3606.036377\n",
            "Epoch Step: 1 Loss: 7.685754 Tokens per Sec: 2862.930176\n",
            "Epoch Step: 51 Loss: 5.287666 Tokens per Sec: 2868.330078\n",
            "Epoch Step: 101 Loss: 5.446177 Tokens per Sec: 2861.213379\n",
            "Epoch Step: 151 Loss: 5.475329 Tokens per Sec: 2868.932129\n",
            "Epoch Step: 201 Loss: 5.460011 Tokens per Sec: 2860.528809\n",
            "Epoch Step: 251 Loss: 5.147444 Tokens per Sec: 2825.586426\n",
            "Epoch Step: 301 Loss: 5.635694 Tokens per Sec: 2887.275879\n",
            "Epoch Step: 1 Loss: 5.467085 Tokens per Sec: 3592.383789\n",
            "Epoch Step: 1 Loss: 7.521385 Tokens per Sec: 2896.160400\n",
            "Epoch Step: 51 Loss: 5.252913 Tokens per Sec: 2862.347900\n",
            "Epoch Step: 101 Loss: 5.412343 Tokens per Sec: 2859.271729\n",
            "Epoch Step: 151 Loss: 5.441571 Tokens per Sec: 2866.161133\n",
            "Epoch Step: 201 Loss: 5.411411 Tokens per Sec: 2859.614502\n",
            "Epoch Step: 251 Loss: 5.122994 Tokens per Sec: 2834.233154\n",
            "Epoch Step: 301 Loss: 5.597674 Tokens per Sec: 2886.929932\n",
            "Epoch Step: 1 Loss: 5.435624 Tokens per Sec: 3567.124023\n",
            "Epoch Step: 1 Loss: 8.152277 Tokens per Sec: 2847.710205\n",
            "Epoch Step: 51 Loss: 5.256604 Tokens per Sec: 2869.347412\n",
            "Epoch Step: 101 Loss: 5.404935 Tokens per Sec: 2859.797119\n",
            "Epoch Step: 151 Loss: 5.420744 Tokens per Sec: 2871.427002\n",
            "Epoch Step: 201 Loss: 5.389226 Tokens per Sec: 2861.932861\n",
            "Epoch Step: 251 Loss: 5.104021 Tokens per Sec: 2853.195557\n",
            "Epoch Step: 301 Loss: 5.579693 Tokens per Sec: 2917.221191\n",
            "Epoch Step: 1 Loss: 5.402404 Tokens per Sec: 3618.393555\n",
            "Epoch Step: 1 Loss: 7.409263 Tokens per Sec: 2904.800537\n",
            "Epoch Step: 51 Loss: 5.210810 Tokens per Sec: 2907.024414\n",
            "Epoch Step: 101 Loss: 5.349298 Tokens per Sec: 2896.080811\n",
            "Epoch Step: 151 Loss: 5.378419 Tokens per Sec: 2901.742188\n",
            "Epoch Step: 201 Loss: 5.372193 Tokens per Sec: 2899.079590\n",
            "Epoch Step: 251 Loss: 5.073011 Tokens per Sec: 2870.601318\n",
            "Epoch Step: 301 Loss: 5.574013 Tokens per Sec: 2922.309326\n",
            "Epoch Step: 1 Loss: 5.375025 Tokens per Sec: 3623.287842\n",
            "Epoch Step: 1 Loss: 7.189966 Tokens per Sec: 2913.212402\n",
            "Epoch Step: 51 Loss: 5.162999 Tokens per Sec: 2907.336670\n",
            "Epoch Step: 101 Loss: 5.338457 Tokens per Sec: 2895.389648\n",
            "Epoch Step: 151 Loss: 5.366870 Tokens per Sec: 2910.502686\n",
            "Epoch Step: 201 Loss: 5.326417 Tokens per Sec: 2899.973633\n",
            "Epoch Step: 251 Loss: 5.060785 Tokens per Sec: 2868.339600\n",
            "Epoch Step: 301 Loss: 5.533745 Tokens per Sec: 2923.097656\n",
            "Epoch Step: 1 Loss: 5.345598 Tokens per Sec: 3629.618408\n",
            "Epoch Step: 1 Loss: 8.238969 Tokens per Sec: 2905.730713\n",
            "Epoch Step: 51 Loss: 5.137515 Tokens per Sec: 2905.277344\n",
            "Epoch Step: 101 Loss: 5.311655 Tokens per Sec: 2896.131836\n",
            "Epoch Step: 151 Loss: 5.353135 Tokens per Sec: 2903.805176\n",
            "Epoch Step: 201 Loss: 5.313359 Tokens per Sec: 2898.843262\n",
            "Epoch Step: 251 Loss: 5.024279 Tokens per Sec: 2868.117920\n",
            "Epoch Step: 301 Loss: 5.524459 Tokens per Sec: 2924.331299\n",
            "Epoch Step: 1 Loss: 5.325972 Tokens per Sec: 3621.479248\n",
            "Epoch Step: 1 Loss: 6.951839 Tokens per Sec: 2906.061279\n",
            "Epoch Step: 51 Loss: 5.167509 Tokens per Sec: 2904.840332\n",
            "Epoch Step: 101 Loss: 5.282578 Tokens per Sec: 2895.880371\n",
            "Epoch Step: 151 Loss: 5.331191 Tokens per Sec: 2906.168213\n",
            "Epoch Step: 201 Loss: 5.281881 Tokens per Sec: 2894.723877\n",
            "Epoch Step: 251 Loss: 5.014883 Tokens per Sec: 2867.776123\n",
            "Epoch Step: 301 Loss: 5.496693 Tokens per Sec: 2920.266113\n",
            "Epoch Step: 1 Loss: 5.292257 Tokens per Sec: 3600.039062\n",
            "['<BOS>', 'the', 'dog', 'ran', 'the', 'first', 'century', ',', 'the', 'first', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk', '>']\n",
            "tensor([[   1,   21, 5135, 1969,   21,   63, 1705,   17,   21,   63,   11,   12,\n",
            "           13,   20,   21,   11,   12,   13,   17,   21,   11,   12,   13,   20,\n",
            "           21,   11,   12,   13,   17,   21,   11,   12,   13]],\n",
            "       device='cuda:0')\n",
            "Epoch Step: 1 Loss: 9.234408 Tokens per Sec: 3290.633301\n",
            "Epoch Step: 51 Loss: 5.123237 Tokens per Sec: 2905.240479\n",
            "Epoch Step: 101 Loss: 5.291457 Tokens per Sec: 2897.187012\n",
            "Epoch Step: 151 Loss: 5.301547 Tokens per Sec: 2904.178223\n",
            "Epoch Step: 201 Loss: 5.245965 Tokens per Sec: 2901.100342\n",
            "Epoch Step: 251 Loss: 4.984142 Tokens per Sec: 2866.030273\n",
            "Epoch Step: 301 Loss: 5.461833 Tokens per Sec: 2924.923340\n",
            "Epoch Step: 1 Loss: 5.265213 Tokens per Sec: 3600.912842\n",
            "Epoch Step: 1 Loss: 7.338897 Tokens per Sec: 2933.132568\n",
            "Epoch Step: 51 Loss: 5.088086 Tokens per Sec: 2903.982666\n",
            "Epoch Step: 101 Loss: 5.236436 Tokens per Sec: 2900.497070\n",
            "Epoch Step: 151 Loss: 5.256579 Tokens per Sec: 2908.838379\n",
            "Epoch Step: 201 Loss: 5.232951 Tokens per Sec: 2899.313965\n",
            "Epoch Step: 251 Loss: 4.945922 Tokens per Sec: 2870.402344\n",
            "Epoch Step: 301 Loss: 5.436030 Tokens per Sec: 2928.272705\n",
            "Epoch Step: 1 Loss: 5.237228 Tokens per Sec: 3645.394287\n",
            "Epoch Step: 1 Loss: 7.472589 Tokens per Sec: 2932.265625\n",
            "Epoch Step: 51 Loss: 5.047160 Tokens per Sec: 2907.374268\n",
            "Epoch Step: 101 Loss: 5.222060 Tokens per Sec: 2900.291016\n",
            "Epoch Step: 151 Loss: 5.233266 Tokens per Sec: 2911.644287\n",
            "Epoch Step: 201 Loss: 5.213663 Tokens per Sec: 2894.260742\n",
            "Epoch Step: 251 Loss: 4.908745 Tokens per Sec: 2869.926025\n",
            "Epoch Step: 301 Loss: 5.404506 Tokens per Sec: 2932.700195\n",
            "Epoch Step: 1 Loss: 5.210522 Tokens per Sec: 3634.502686\n",
            "Epoch Step: 1 Loss: 8.362581 Tokens per Sec: 2942.287598\n",
            "Epoch Step: 51 Loss: 5.018441 Tokens per Sec: 2903.597656\n",
            "Epoch Step: 101 Loss: 5.193003 Tokens per Sec: 2904.025146\n",
            "Epoch Step: 151 Loss: 5.218807 Tokens per Sec: 2904.594238\n",
            "Epoch Step: 201 Loss: 5.189459 Tokens per Sec: 2897.780273\n",
            "Epoch Step: 251 Loss: 4.911887 Tokens per Sec: 2866.781494\n",
            "Epoch Step: 301 Loss: 5.393704 Tokens per Sec: 2922.843750\n",
            "Epoch Step: 1 Loss: 5.176892 Tokens per Sec: 3599.577637\n",
            "Epoch Step: 1 Loss: 7.563669 Tokens per Sec: 2921.248047\n",
            "Epoch Step: 51 Loss: 4.991221 Tokens per Sec: 2902.863037\n",
            "Epoch Step: 101 Loss: 5.166226 Tokens per Sec: 2900.612549\n",
            "Epoch Step: 151 Loss: 5.194548 Tokens per Sec: 2906.800781\n",
            "Epoch Step: 201 Loss: 5.142776 Tokens per Sec: 2899.922607\n",
            "Epoch Step: 251 Loss: 4.895994 Tokens per Sec: 2870.091309\n",
            "Epoch Step: 301 Loss: 5.366029 Tokens per Sec: 2925.719971\n",
            "Epoch Step: 1 Loss: 5.147809 Tokens per Sec: 3635.488525\n",
            "Epoch Step: 1 Loss: 7.959086 Tokens per Sec: 2928.751221\n",
            "Epoch Step: 51 Loss: 4.968615 Tokens per Sec: 2909.158691\n",
            "Epoch Step: 101 Loss: 5.144129 Tokens per Sec: 2897.799561\n",
            "Epoch Step: 151 Loss: 5.166602 Tokens per Sec: 2911.338623\n",
            "Epoch Step: 201 Loss: 5.144931 Tokens per Sec: 2900.276611\n",
            "Epoch Step: 251 Loss: 4.862631 Tokens per Sec: 2873.407959\n",
            "Epoch Step: 301 Loss: 5.342492 Tokens per Sec: 2928.545898\n",
            "Epoch Step: 1 Loss: 5.118630 Tokens per Sec: 3654.043457\n",
            "Epoch Step: 1 Loss: 8.122719 Tokens per Sec: 2920.043213\n",
            "Epoch Step: 51 Loss: 4.942990 Tokens per Sec: 2908.797607\n",
            "Epoch Step: 101 Loss: 5.120625 Tokens per Sec: 2896.357178\n",
            "Epoch Step: 151 Loss: 5.129210 Tokens per Sec: 2910.194336\n",
            "Epoch Step: 201 Loss: 5.103409 Tokens per Sec: 2898.485840\n",
            "Epoch Step: 251 Loss: 4.846710 Tokens per Sec: 2869.209473\n",
            "Epoch Step: 301 Loss: 5.306475 Tokens per Sec: 2922.207275\n",
            "Epoch Step: 1 Loss: 5.086022 Tokens per Sec: 3635.427490\n",
            "Epoch Step: 1 Loss: 8.458645 Tokens per Sec: 2898.224609\n",
            "Epoch Step: 51 Loss: 4.892890 Tokens per Sec: 2906.376221\n",
            "Epoch Step: 101 Loss: 5.094644 Tokens per Sec: 2900.020752\n",
            "Epoch Step: 151 Loss: 5.121998 Tokens per Sec: 2911.329590\n",
            "Epoch Step: 201 Loss: 5.069726 Tokens per Sec: 2903.184814\n",
            "Epoch Step: 251 Loss: 4.814048 Tokens per Sec: 2874.208984\n",
            "Epoch Step: 301 Loss: 5.290114 Tokens per Sec: 2926.436035\n",
            "Epoch Step: 1 Loss: 5.054602 Tokens per Sec: 3616.706543\n",
            "Epoch Step: 1 Loss: 7.310422 Tokens per Sec: 2923.979980\n",
            "Epoch Step: 51 Loss: 4.894306 Tokens per Sec: 2907.395508\n",
            "Epoch Step: 101 Loss: 5.072210 Tokens per Sec: 2895.568115\n",
            "Epoch Step: 151 Loss: 5.100507 Tokens per Sec: 2910.799561\n",
            "Epoch Step: 201 Loss: 5.057998 Tokens per Sec: 2901.960205\n",
            "Epoch Step: 251 Loss: 4.789073 Tokens per Sec: 2872.046631\n",
            "Epoch Step: 301 Loss: 5.263890 Tokens per Sec: 2922.060059\n",
            "Epoch Step: 1 Loss: 5.024722 Tokens per Sec: 3628.151367\n",
            "Epoch Step: 1 Loss: 7.729396 Tokens per Sec: 2913.668945\n",
            "Epoch Step: 51 Loss: 4.839954 Tokens per Sec: 2909.151611\n",
            "Epoch Step: 101 Loss: 5.031728 Tokens per Sec: 2899.783203\n",
            "Epoch Step: 151 Loss: 5.078439 Tokens per Sec: 2904.330078\n",
            "Epoch Step: 201 Loss: 5.035854 Tokens per Sec: 2896.572266\n",
            "Epoch Step: 251 Loss: 4.767765 Tokens per Sec: 2860.953613\n",
            "Epoch Step: 301 Loss: 5.231400 Tokens per Sec: 2921.094971\n",
            "Epoch Step: 1 Loss: 4.987523 Tokens per Sec: 3599.001465\n",
            "['<BOS>', 'the', 'dog', 'ran', 'by', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', 'of', 'the', '<', 'unk', '>', ',', 'the', '<', 'unk']\n",
            "tensor([[   1,   21, 5135, 1969,   39,   21,   11,   12,   13,   20,   21,   11,\n",
            "           12,   13,   20,   21,   11,   12,   13,   20,   21,   11,   12,   13,\n",
            "           20,   21,   11,   12,   13,   17,   21,   11,   12]],\n",
            "       device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.829238 Tokens per Sec: 3264.474121\n",
            "Epoch Step: 51 Loss: 4.804045 Tokens per Sec: 2906.375000\n",
            "Epoch Step: 101 Loss: 5.004297 Tokens per Sec: 2900.955322\n",
            "Epoch Step: 151 Loss: 5.049030 Tokens per Sec: 2906.979004\n",
            "Epoch Step: 201 Loss: 5.019307 Tokens per Sec: 2901.488037\n",
            "Epoch Step: 251 Loss: 4.730813 Tokens per Sec: 2869.677979\n",
            "Epoch Step: 301 Loss: 5.203779 Tokens per Sec: 2929.518799\n",
            "Epoch Step: 1 Loss: 4.956266 Tokens per Sec: 3622.498535\n",
            "Epoch Step: 1 Loss: 6.911371 Tokens per Sec: 2942.980713\n",
            "Epoch Step: 51 Loss: 4.792521 Tokens per Sec: 2905.045166\n",
            "Epoch Step: 101 Loss: 4.980437 Tokens per Sec: 2900.163818\n",
            "Epoch Step: 151 Loss: 5.030581 Tokens per Sec: 2910.267334\n",
            "Epoch Step: 201 Loss: 4.981553 Tokens per Sec: 2898.826660\n",
            "Epoch Step: 251 Loss: 4.718973 Tokens per Sec: 2872.708496\n",
            "Epoch Step: 301 Loss: 5.180179 Tokens per Sec: 2930.797119\n",
            "Epoch Step: 1 Loss: 4.921603 Tokens per Sec: 3637.635742\n",
            "Epoch Step: 1 Loss: 7.261893 Tokens per Sec: 2952.093506\n",
            "Epoch Step: 51 Loss: 4.772849 Tokens per Sec: 2910.547852\n",
            "Epoch Step: 101 Loss: 4.963642 Tokens per Sec: 2899.767334\n",
            "Epoch Step: 151 Loss: 4.989033 Tokens per Sec: 2910.969971\n",
            "Epoch Step: 201 Loss: 4.945259 Tokens per Sec: 2902.151367\n",
            "Epoch Step: 251 Loss: 4.700414 Tokens per Sec: 2874.134033\n",
            "Epoch Step: 301 Loss: 5.126669 Tokens per Sec: 2925.492188\n",
            "Epoch Step: 1 Loss: 4.884384 Tokens per Sec: 3613.906250\n",
            "Epoch Step: 1 Loss: 6.971407 Tokens per Sec: 2870.072998\n",
            "Epoch Step: 51 Loss: 4.732444 Tokens per Sec: 2883.071533\n",
            "Epoch Step: 101 Loss: 4.928403 Tokens per Sec: 2863.736328\n",
            "Epoch Step: 151 Loss: 4.976610 Tokens per Sec: 2871.913574\n",
            "Epoch Step: 201 Loss: 4.919769 Tokens per Sec: 2859.379639\n",
            "Epoch Step: 251 Loss: 4.664463 Tokens per Sec: 2830.613281\n",
            "Epoch Step: 301 Loss: 5.120387 Tokens per Sec: 2881.072754\n",
            "Epoch Step: 1 Loss: 4.845586 Tokens per Sec: 3539.054199\n",
            "Epoch Step: 1 Loss: 6.804484 Tokens per Sec: 2862.868164\n",
            "Epoch Step: 51 Loss: 4.705065 Tokens per Sec: 2866.084717\n",
            "Epoch Step: 101 Loss: 4.909726 Tokens per Sec: 2860.292725\n",
            "Epoch Step: 151 Loss: 4.935302 Tokens per Sec: 2868.328369\n",
            "Epoch Step: 201 Loss: 4.888487 Tokens per Sec: 2857.212158\n",
            "Epoch Step: 251 Loss: 4.649079 Tokens per Sec: 2829.040771\n",
            "Epoch Step: 301 Loss: 5.097850 Tokens per Sec: 2890.160156\n",
            "Epoch Step: 1 Loss: 4.809485 Tokens per Sec: 3587.123779\n",
            "Epoch Step: 1 Loss: 6.615073 Tokens per Sec: 2873.042480\n",
            "Epoch Step: 51 Loss: 4.663408 Tokens per Sec: 2867.046631\n",
            "Epoch Step: 101 Loss: 4.871433 Tokens per Sec: 2857.472412\n",
            "Epoch Step: 151 Loss: 4.883137 Tokens per Sec: 2869.054688\n",
            "Epoch Step: 201 Loss: 4.854809 Tokens per Sec: 2862.697998\n",
            "Epoch Step: 251 Loss: 4.613809 Tokens per Sec: 2832.219971\n",
            "Epoch Step: 301 Loss: 5.054538 Tokens per Sec: 2888.881592\n",
            "Epoch Step: 1 Loss: 4.771630 Tokens per Sec: 3589.822021\n",
            "Epoch Step: 1 Loss: 7.056267 Tokens per Sec: 2870.978027\n",
            "Epoch Step: 51 Loss: 4.654517 Tokens per Sec: 2872.378906\n",
            "Epoch Step: 101 Loss: 4.844431 Tokens per Sec: 2859.995605\n",
            "Epoch Step: 151 Loss: 4.895509 Tokens per Sec: 2868.440186\n",
            "Epoch Step: 201 Loss: 4.813782 Tokens per Sec: 2855.906006\n",
            "Epoch Step: 251 Loss: 4.591342 Tokens per Sec: 2830.393066\n",
            "Epoch Step: 301 Loss: 5.039208 Tokens per Sec: 2885.282227\n",
            "Epoch Step: 1 Loss: 4.728869 Tokens per Sec: 3557.712158\n",
            "Epoch Step: 1 Loss: 7.404463 Tokens per Sec: 2878.191162\n",
            "Epoch Step: 51 Loss: 4.620382 Tokens per Sec: 2861.973877\n",
            "Epoch Step: 101 Loss: 4.809807 Tokens per Sec: 2856.044434\n",
            "Epoch Step: 151 Loss: 4.828320 Tokens per Sec: 2870.676514\n",
            "Epoch Step: 201 Loss: 4.794631 Tokens per Sec: 2854.217773\n",
            "Epoch Step: 251 Loss: 4.562328 Tokens per Sec: 2828.206787\n",
            "Epoch Step: 301 Loss: 5.007305 Tokens per Sec: 2883.946533\n",
            "Epoch Step: 1 Loss: 4.689858 Tokens per Sec: 3586.436035\n",
            "Epoch Step: 1 Loss: 6.452451 Tokens per Sec: 2904.482422\n",
            "Epoch Step: 51 Loss: 4.579359 Tokens per Sec: 2864.756592\n",
            "Epoch Step: 101 Loss: 4.774554 Tokens per Sec: 2859.730957\n",
            "Epoch Step: 151 Loss: 4.808618 Tokens per Sec: 2868.294922\n",
            "Epoch Step: 201 Loss: 4.758860 Tokens per Sec: 2860.193848\n",
            "Epoch Step: 251 Loss: 4.526578 Tokens per Sec: 2831.136230\n",
            "Epoch Step: 301 Loss: 4.965930 Tokens per Sec: 2887.233887\n",
            "Epoch Step: 1 Loss: 4.656188 Tokens per Sec: 3587.389160\n",
            "Epoch Step: 1 Loss: 7.050490 Tokens per Sec: 2896.898926\n",
            "Epoch Step: 51 Loss: 4.570547 Tokens per Sec: 2880.666748\n",
            "Epoch Step: 101 Loss: 4.764899 Tokens per Sec: 2872.462891\n",
            "Epoch Step: 151 Loss: 4.784578 Tokens per Sec: 2871.809814\n",
            "Epoch Step: 201 Loss: 4.706903 Tokens per Sec: 2860.338623\n",
            "Epoch Step: 251 Loss: 4.510336 Tokens per Sec: 2828.866699\n",
            "Epoch Step: 301 Loss: 4.916298 Tokens per Sec: 2879.370605\n",
            "Epoch Step: 1 Loss: 4.608052 Tokens per Sec: 3568.241699\n",
            "['<BOS>', 'the', 'dog', 'ran', 'a', 'tropical', 'storm', ',', 'the', 'storm', 'was', 'the', '<', 'unk', '>', ',', 'and', 'the', '<', 'unk', '>', ',', '<', 'unk', '>', ',', '<', 'unk', '>', ',', '<', 'unk', '>']\n",
            "tensor([[    1,    21,  5135,  1969,    31, 11006,  3275,    17,    21,  3275,\n",
            "           133,    21,    11,    12,    13,    17,    41,    21,    11,    12,\n",
            "            13,    17,    11,    12,    13,    17,    11,    12,    13,    17,\n",
            "            11,    12,    13]], device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.641157 Tokens per Sec: 3242.844482\n",
            "Epoch Step: 51 Loss: 4.494514 Tokens per Sec: 2860.435059\n",
            "Epoch Step: 101 Loss: 4.745988 Tokens per Sec: 2858.104492\n",
            "Epoch Step: 151 Loss: 4.759897 Tokens per Sec: 2870.327881\n",
            "Epoch Step: 201 Loss: 4.680401 Tokens per Sec: 2862.540771\n",
            "Epoch Step: 251 Loss: 4.504186 Tokens per Sec: 2833.055908\n",
            "Epoch Step: 301 Loss: 4.884128 Tokens per Sec: 2885.965332\n",
            "Epoch Step: 1 Loss: 4.561193 Tokens per Sec: 3573.768066\n",
            "Epoch Step: 1 Loss: 7.449096 Tokens per Sec: 2903.354004\n",
            "Epoch Step: 51 Loss: 4.500263 Tokens per Sec: 2870.174072\n",
            "Epoch Step: 101 Loss: 4.674283 Tokens per Sec: 2858.418457\n",
            "Epoch Step: 151 Loss: 4.708395 Tokens per Sec: 2866.271729\n",
            "Epoch Step: 201 Loss: 4.652379 Tokens per Sec: 2859.813477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKwBYlGKcqIQ"
      },
      "source": [
        "model.eval()\n",
        "sentence_start = ['<BOS>', 'the', 'dog', 'ran']\n",
        "symbol_list = [voc_w2.to_index(token) for token in sentence_start]\n",
        "output = greedy_decode(model, 30, symbol_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}